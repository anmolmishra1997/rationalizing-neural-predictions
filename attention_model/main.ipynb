{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import random\n",
    "\n",
    "import AttnEncoder\n",
    "# Global defs\n",
    "\n",
    "# iters_per_epoch should also be shifted here ?\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "batch_size = 32\n",
    "\n",
    "MASTER_MAX_LEN = 300\n",
    "MASTER_MAX_VAL_LEN = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function defs\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def asMinutes(s):\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "\tnow = time.time()\n",
    "\ts = now - since\n",
    "\tes = s / (percent + 1e-8)\n",
    "\trs = es - s\n",
    "\treturn '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ** Starting of the Main code \n",
    "# ** ** \n",
    "# ** ** \n",
    "import re\n",
    "\n",
    "# Convert string to vector of floats\n",
    "def convert_to_float(string): # string with float values separated by spaces\n",
    "\tlis = string.split()\n",
    "\tlis_rating = [ float(value) for value in lis]\n",
    "\treturn lis_rating\n",
    "\n",
    "# Unique index for words\n",
    "index = 0\n",
    "def get_index():\n",
    "\tglobal index\n",
    "\tto_ret = index\n",
    "\tindex += 1\n",
    "\treturn to_ret\n",
    "\n",
    "# Dictionaries\n",
    "dict_ind2vec = {}\n",
    "dict_ind2str = {}\n",
    "dict_str2ind = {}\n",
    "\n",
    "def get_list_of_indices(string):\n",
    "\tlis_words = string.split()\n",
    "\t# lis_ret = [ for word in lis_words]\n",
    "\tlis_ret = []\n",
    "\tfor word in lis_words:\n",
    "\t\ttry:\n",
    "\t\t\tind_append = dict_str2ind[word]\n",
    "\t\t\tlis_ret.append(ind_append)\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\t\t\t# ind_append = \n",
    "\t\t\t# print(\"THERE IT IS!\", word)\n",
    "\t# print(\"About to return\")\n",
    "\treturn lis_ret\n",
    "\n",
    "## **\n",
    "## **\n",
    "# read the word2vec representations\n",
    "\n",
    "with open('../review+wiki.filtered.200.txt') as f:\n",
    "\twordvecs = f.readlines()\n",
    "\n",
    "first_pair = wordvecs[0].split(\" \", 1)\n",
    "first_vec = convert_to_float(first_pair[1])\n",
    "dim_vecSpace = len(first_vec) # Dimension of the vector space in which we are\n",
    "\n",
    "# add stuff for EOS, Blank\n",
    "# at index = 0, 1\n",
    "\n",
    "eos_index = get_index()\n",
    "dict_str2ind[\"<EOS>\"] = eos_index\n",
    "dict_ind2str[eos_index] = \"<EOS>\"\n",
    "dict_ind2vec[eos_index] = [1.0]*dim_vecSpace\n",
    "\n",
    "blk_index = get_index()\n",
    "dict_str2ind[\"<BLANK>\"] = blk_index\n",
    "dict_ind2str[blk_index] = \"<BLANK>\"\n",
    "dict_ind2vec[blk_index] = [0.0]*dim_vecSpace\n",
    "\n",
    "\n",
    "for elem in wordvecs:\n",
    "\tliss = elem.split(\" \", 1) # split on the first space\n",
    "\tword_str = liss[0]\n",
    "\tword_vec = convert_to_float(liss[1])\n",
    "\t\n",
    "\there_index = get_index()\n",
    "\tdict_str2ind[word_str] = here_index\n",
    "\tdict_ind2str[here_index] = word_str\n",
    "\tdict_ind2vec[here_index] = word_vec\n",
    "\n",
    "# CHKING\n",
    "# print( dict_str2ind['a'] )\n",
    "\n",
    "## **\n",
    "## **\n",
    "# read the data\n",
    "\n",
    "with open('../reviews.aspect0.train.txt') as f:\n",
    "\ttrain_data = f.readlines()\n",
    "\n",
    "rating_regex = re.compile('\\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d\\t') # Exactly matches only the ratings\n",
    "\n",
    "# extract ratings - # each rating is a scalar value # NO ::: each rating is a list of 5 values\n",
    "ratings = [ float( re.findall(rating_regex, review)[0].split()[0] ) for review in train_data ]\n",
    "\n",
    "# extract reviews\n",
    "reviews_str = [ rating_regex.sub('', review) for review in train_data ]\n",
    "reviews = [ get_list_of_indices( review_str ) for review_str in reviews_str ]\n",
    "X = reviews\n",
    "total_size = len(X)\n",
    "\n",
    "divide_train = int( (4*total_size)/5 )\n",
    "train_indices_of_X = sorted( random.sample( range(total_size), divide_train ) )\n",
    "\n",
    "X_train = []\n",
    "X_val = []\n",
    "ratings_train = []\n",
    "ratings_val = []\n",
    "for i in range(total_size):\n",
    "\tif i in train_indices_of_X:\n",
    "\t\tX_train.append(X[i])\n",
    "\t\tratings_train.append(ratings[i])\n",
    "\telse:\n",
    "\t\tX_val.append(X[i])\n",
    "\t\tratings_val.append(ratings[i])\n",
    "\n",
    "X = X_train\n",
    "ratings = ratings_train\n",
    "\n",
    "num_train_examples = len(X) # we also assume len(X) = len(ratings)\n",
    "num_val_examples = len(X_val)\n",
    "# read validation data\n",
    "\n",
    "# ** ** \n",
    "# ** ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy(X, ratings, attn_encoder):\n",
    "\n",
    "    # iterate through X_val and pass to generator->encoder to get mse_error and compare it to truth\n",
    "    num_val_examples = len(X)\n",
    "    X_val_size = num_val_examples\n",
    "    num_iters = X_val_size // (batch_size)\n",
    "    total_loss = 0.0\n",
    "    slice_index = 0\n",
    "    for iters in range(num_iters):\n",
    "\n",
    "        # get X_batch, ratings_batch\n",
    "        # This sampling also preserves the order\n",
    "#         X_bch = []\n",
    "#         ratings_bch = []\n",
    "\n",
    "#         _ = [ ( X_bch.append(X[i]) , ratings_bch.append(ratings[i]) ) for i in sorted(random.sample(range(num_val_examples), batch_size)) ]\n",
    "        \n",
    "        X_bch = X[slice_index : slice_index + batch_size]\n",
    "        ratings_bch = ratings[slice_index : slice_index + batch_size]\n",
    "        slice_index += batch_size\n",
    "        # almost done here - make all the reviews of equal length now\n",
    "\n",
    "        maxlen_rev = max(X_bch, key=len)\n",
    "        maxlen = len(maxlen_rev)\n",
    "\n",
    "        max_seq_len = min(maxlen, MASTER_MAX_VAL_LEN)\n",
    "        \n",
    "        \n",
    "        X_bach = np.empty([batch_size,max_seq_len])\n",
    "        ratings_bach = np.empty([batch_size,1])\n",
    "        \n",
    "        encoderLoss = nn.MSELoss(reduce=False)\n",
    "\n",
    "        for iterr in range(batch_size):\n",
    "            currentlen = len(X_bch[iterr])\n",
    "            if (currentlen < max_seq_len):\n",
    "                zero_count = max_seq_len - currentlen\n",
    "                X_bch[iterr].extend([0]*zero_count)\n",
    "            else:\n",
    "                X_bch[iterr] = X_bch[iterr][0:max_seq_len]\n",
    "            # X_bch[iterr] is now a list containing indices of words\n",
    "            # Convert it into a Variable ?\n",
    "            to_append = np.array( X_bch[iterr] )\n",
    "            X_bach[iterr] = to_append\n",
    "            to_append = np.array( ratings_bch[iterr] )\n",
    "            ratings_bach[iterr] = to_append\n",
    "        # X_bach is a 2d numpy array of size :: batch_size X maxlen\n",
    "\n",
    "        if (use_cuda):\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor).cuda()\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor).cuda()\n",
    "        else:\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor)\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor)\n",
    "\n",
    "        X_batch = Variable(X_bach_tensor)\n",
    "        ratings_batch = Variable(ratings_bach_tensor)\n",
    "\n",
    "#         init_hidden = generator.initHidden(batch_size, use_cuda)\n",
    "#         z_sample = generator.sample(X_batch, init_hidden, use_cuda)\n",
    "\n",
    "        attn_weights, ratings_pred = attn_encoder(X_batch, use_cuda, False)\n",
    "        encoder_loss = encoderLoss(ratings_pred, ratings_batch.squeeze(1))\n",
    "\n",
    "        total_loss += float(torch.sum(encoder_loss))\n",
    "\n",
    "    return total_loss / X_val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train function - here's some ingenuity\n",
    "# one iteration of training\n",
    "def train(X, ratings, attn_encoder, attn_encoder_optimizer, print_grad_norm):\n",
    "    # X - single batch\n",
    "\n",
    "    attn_encoder_optimizer.zero_grad()\n",
    "\n",
    "    encoderLoss = nn.MSELoss(reduce=False)\n",
    "\n",
    "    mean_cost = 0.0\n",
    "\n",
    "    attn_weights, ratings_pred = attn_encoder(X, use_cuda)\n",
    "    encoder_loss = encoderLoss(ratings_pred, ratings.squeeze(1))\n",
    "\n",
    "    cost = encoder_loss\n",
    "       \n",
    "    cost1 = torch.mean(cost)\n",
    "\n",
    "    mean_cost += float(cost1)\n",
    "\n",
    "    cost1.backward()\n",
    "        \n",
    "        \n",
    "    if (print_grad_norm):\n",
    "        for name, param in attn_encoder.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "                print(param.data.norm())\n",
    "                print (param.grad.data.norm())\n",
    "                \n",
    "        input()\n",
    "    \n",
    "    attn_encoder_optimizer.step()\n",
    "    \n",
    "    return mean_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(X, ratings, X_val, ratings_val, attn_encoder, learning_rate, learning_rate_decay, num_epochs, \\\n",
    "                load_dict=None, print_every=1000, plot_every=100, val_every=1000, print_grad_every=-1, \\\n",
    "                save_folder='', weight_decay=0):\n",
    "\n",
    "    \n",
    "    num_train_examples = len(X)\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0.0\n",
    "    plot_loss_total = 0.0\n",
    "    best_val_acc = float(\"inf\")\n",
    "    if load_dict is not None:\n",
    "        attn_encoder = load_dict['attn_encoder_model']\n",
    "        \n",
    "        cur_tot_iters = load_dict['tot_iter']\n",
    "    else:\n",
    "        cur_tot_iters = 0\n",
    "        \n",
    "    attn_enc_param_list = []\n",
    "    for param in attn_encoder.parameters():\n",
    "        if (param.requires_grad):\n",
    "            attn_enc_param_list.append(param)\n",
    "    attn_encoder_optimizer = optim.Adam(attn_enc_param_list, lr=learning_rate, weight_decay=weight_decay)    \n",
    "        \n",
    "    attn_encoder_scheduler = optim.lr_scheduler.StepLR(attn_encoder_optimizer, 1, learning_rate_decay)\n",
    "    \n",
    "    if (load_dict is not None):\n",
    "        attn_encoder_optimizer.load_state_dict(load_dict['attn_encoder_optimizer'])\n",
    "        for param_group in attn_encoder_optimizer.param_groups:\n",
    "            param_group['weight_decay'] = weight_decay\n",
    "            param_group['lr'] = learning_rate\n",
    "            \n",
    "#         attn_encoder_scheduler.load_state_dict(load_dict['attn_encoder_scheduler'])\n",
    "\n",
    "#     print(encoder_optimizer.param_groups[0]['lr'])\n",
    "#      set iters_per_epoch\n",
    "    iters_per_epoch = num_train_examples // batch_size\n",
    "    n_iters = iters_per_epoch * num_epochs\n",
    "    \n",
    "    position_set = False\n",
    "    for epoch in range(num_epochs):\n",
    "        if (position_set):\n",
    "            pass\n",
    "#             encoder_scheduler.step()\n",
    "#             generator_scheduler.step()\n",
    "        for iter_num in range(iters_per_epoch):\n",
    "            if (cur_tot_iters >= epoch * iters_per_epoch + iter_num + 1):\n",
    "                continue\n",
    "            \n",
    "            position_set = True\n",
    "            # randomly choose sample from X and make them equal length\n",
    "            # This sampling also preserves the order\n",
    "            X_bch = []\n",
    "            ratings_bch = []\n",
    "\n",
    "            _ = [ ( X_bch.append(X[i]) , ratings_bch.append(ratings[i]) ) for i in sorted(random.sample(range(num_train_examples), batch_size)) ]\n",
    "\n",
    "            # almost done here - make all the reviews of equal length now\n",
    "\n",
    "            maxlen_rev = max(X_bch, key=len)\n",
    "            maxlen = len(maxlen_rev)\n",
    "\n",
    "            max_seq_len = min(maxlen, MASTER_MAX_LEN)\n",
    "            \n",
    "            X_bach = np.empty([batch_size,max_seq_len])\n",
    "            ratings_bach = np.empty([batch_size,1])\n",
    "\n",
    "            for iterr in range(batch_size):\n",
    "                currentlen = len(X_bch[iterr])\n",
    "                if (currentlen < max_seq_len):\n",
    "                    zero_count = max_seq_len - currentlen\n",
    "                    X_bch[iterr].extend([0]*zero_count)\n",
    "                else:\n",
    "                    X_bch[iterr] = X_bch[iterr][0:max_seq_len]\n",
    "                # X_bch[iterr] is now a list containing indices of words\n",
    "                # Convert it into a Variable ?\n",
    "                to_append = np.array( X_bch[iterr] )\n",
    "    # \t\t\t\tX_bach = np.append(X_bach, [to_append], axis = 0)\n",
    "                X_bach[iterr] = to_append\n",
    "                to_append = np.array( ratings_bch[iterr] )\n",
    "    # \t\t\t\tratings_bach = np.append(ratings_bach, to_append, axis = 0)\n",
    "                ratings_bach[iterr] = to_append\n",
    "            # X_bach is a 2d numpy array of size :: batch_size X maxlen\n",
    "\n",
    "            if (use_cuda):\n",
    "                X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor).cuda()\n",
    "                ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor).cuda()\n",
    "            else:\n",
    "                X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor)\n",
    "                ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor)\n",
    "            X_batch = Variable(X_bach_tensor)\n",
    "            ratings_batch = Variable(ratings_bach_tensor)\n",
    "            # call train with this batch\n",
    "            cur_tot_iters = iter_num + 1 + epoch * iters_per_epoch\n",
    "            if (print_grad_every > 0 and cur_tot_iters % print_grad_every == 0):\n",
    "                cur_loss = train(X_batch, ratings_batch, attn_encoder, \\\n",
    "                                                   attn_encoder_optimizer, True)\n",
    "            else:\n",
    "                cur_loss = train(X_batch, ratings_batch, attn_encoder, \\\n",
    "                                                   attn_encoder_optimizer, False)\n",
    "            \n",
    "            print_loss_total += cur_loss\n",
    "            plot_loss_total += cur_loss\n",
    "\n",
    "            \n",
    "          \n",
    "            if (cur_tot_iters) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, 1.0 * (cur_tot_iters) / n_iters),\n",
    "                                             cur_tot_iters, 1.0 * (cur_tot_iters) / n_iters * 100, print_loss_avg),flush=True)\n",
    "            \n",
    "            if (cur_tot_iters) % val_every == 0:\n",
    "                val_acc = getAccuracy(X_val, ratings_val, attn_encoder)\n",
    "                print(\"Val Acc: \", val_acc)\n",
    "                if (val_acc < best_val_acc):\n",
    "                    best_val_acc = val_acc\n",
    "                    best = True\n",
    "                else:\n",
    "                    best = False\n",
    "                    \n",
    "                save_dict = {}\n",
    "                save_dict['attn_encoder_model'] = attn_encoder\n",
    "                save_dict['attn_encoder_optimizer'] = attn_encoder_optimizer.state_dict()\n",
    "#                 save_dict['attn_encoder_scheduler'] = attn_encoder_scheduler.state_dict()\n",
    "                \n",
    "                save_dict['tot_iter'] = cur_tot_iters\n",
    "                save_dict['val_acc'] = val_acc\n",
    "                save_dict['best_so_far'] = best\n",
    "                torch.save(save_dict, save_folder+'chkpt_'+str(cur_tot_iters)+str(best))\n",
    "\n",
    "            if iter_num % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining pretrained_embeddings\n",
    "pretrained_embeddings = np.empty([len(dict_ind2vec), dim_vecSpace])\n",
    "for key in sorted(dict_ind2vec.keys()):\n",
    "    vec_here = dict_ind2vec[key]\n",
    "    pretrained_embeddings[key] = np.array(vec_here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhavya/temp/attn_model/AttnEncoder.py:49: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  hidden, (h_n, c_n) = self.lstm_i2h(x_transpose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 3s (- 0m 2s) (28100 56%) 0.0189\n",
      "0m 6s (- 0m 5s) (28200 56%) 0.0195\n",
      "0m 9s (- 0m 7s) (28300 56%) 0.0196\n",
      "0m 12s (- 0m 9s) (28400 56%) 0.0196\n",
      "0m 15s (- 0m 11s) (28500 56%) 0.0193\n",
      "0m 18s (- 0m 13s) (28600 57%) 0.0196\n",
      "0m 21s (- 0m 15s) (28700 57%) 0.0191\n",
      "0m 24s (- 0m 17s) (28800 57%) 0.0198\n",
      "0m 27s (- 0m 19s) (28900 57%) 0.0192\n",
      "0m 30s (- 0m 21s) (29000 57%) 0.0192\n",
      "Val Acc:  0.01977469354867935\n",
      "0m 39s (- 0m 28s) (29100 58%) 0.0204\n",
      "0m 42s (- 0m 30s) (29200 58%) 0.0189\n",
      "0m 45s (- 0m 32s) (29300 58%) 0.0193\n",
      "0m 48s (- 0m 34s) (29400 58%) 0.0198\n",
      "0m 52s (- 0m 36s) (29500 59%) 0.0192\n",
      "0m 55s (- 0m 38s) (29600 59%) 0.0196\n",
      "0m 58s (- 0m 40s) (29700 59%) 0.0194\n",
      "1m 2s (- 0m 42s) (29800 59%) 0.0194\n",
      "1m 5s (- 0m 44s) (29900 59%) 0.0199\n",
      "1m 8s (- 0m 45s) (30000 60%) 0.0203\n",
      "Val Acc:  0.019690946977585555\n",
      "1m 18s (- 0m 51s) (30100 60%) 0.0184\n",
      "1m 21s (- 0m 53s) (30200 60%) 0.0190\n",
      "1m 25s (- 0m 55s) (30300 60%) 0.0197\n",
      "1m 28s (- 0m 57s) (30400 60%) 0.0187\n",
      "1m 32s (- 0m 59s) (30500 61%) 0.0197\n",
      "1m 35s (- 1m 0s) (30600 61%) 0.0195\n",
      "1m 39s (- 1m 2s) (30700 61%) 0.0194\n",
      "1m 42s (- 1m 3s) (30800 61%) 0.0194\n",
      "1m 46s (- 1m 5s) (30900 61%) 0.0184\n",
      "1m 50s (- 1m 7s) (31000 62%) 0.0184\n",
      "Val Acc:  0.019680824268609284\n",
      "1m 58s (- 1m 12s) (31100 62%) 0.0188\n",
      "2m 2s (- 1m 13s) (31200 62%) 0.0190\n",
      "2m 5s (- 1m 14s) (31300 62%) 0.0194\n",
      "2m 8s (- 1m 16s) (31400 62%) 0.0203\n",
      "2m 11s (- 1m 17s) (31500 63%) 0.0196\n",
      "2m 15s (- 1m 18s) (31600 63%) 0.0185\n",
      "2m 18s (- 1m 20s) (31700 63%) 0.0195\n",
      "2m 22s (- 1m 21s) (31800 63%) 0.0201\n",
      "2m 25s (- 1m 22s) (31900 63%) 0.0190\n",
      "2m 29s (- 1m 23s) (32000 64%) 0.0197\n",
      "Val Acc:  0.019654405571520327\n",
      "2m 37s (- 1m 27s) (32100 64%) 0.0194\n",
      "2m 40s (- 1m 28s) (32200 64%) 0.0193\n",
      "2m 43s (- 1m 29s) (32300 64%) 0.0194\n",
      "2m 47s (- 1m 30s) (32400 64%) 0.0182\n",
      "2m 51s (- 1m 32s) (32500 65%) 0.0190\n",
      "2m 54s (- 1m 33s) (32600 65%) 0.0192\n",
      "2m 58s (- 1m 34s) (32700 65%) 0.0192\n",
      "3m 2s (- 1m 35s) (32800 65%) 0.0194\n",
      "3m 5s (- 1m 36s) (32900 65%) 0.0188\n",
      "3m 9s (- 1m 37s) (33000 66%) 0.0193\n",
      "Val Acc:  0.01966375420242548\n",
      "3m 18s (- 1m 41s) (33100 66%) 0.0192\n",
      "3m 22s (- 1m 42s) (33200 66%) 0.0189\n",
      "3m 25s (- 1m 43s) (33300 66%) 0.0193\n",
      "3m 29s (- 1m 43s) (33400 66%) 0.0190\n",
      "3m 32s (- 1m 44s) (33500 67%) 0.0187\n",
      "3m 36s (- 1m 45s) (33600 67%) 0.0187\n",
      "3m 40s (- 1m 46s) (33700 67%) 0.0187\n",
      "3m 43s (- 1m 47s) (33800 67%) 0.0192\n",
      "3m 47s (- 1m 48s) (33900 67%) 0.0190\n",
      "3m 51s (- 1m 48s) (34000 68%) 0.0202\n",
      "Val Acc:  0.019626462794840337\n",
      "4m 0s (- 1m 51s) (34100 68%) 0.0195\n",
      "4m 3s (- 1m 52s) (34200 68%) 0.0191\n",
      "4m 6s (- 1m 52s) (34300 68%) 0.0194\n",
      "4m 9s (- 1m 53s) (34400 68%) 0.0196\n",
      "4m 12s (- 1m 53s) (34500 69%) 0.0188\n",
      "4m 16s (- 1m 54s) (34600 69%) 0.0192\n",
      "4m 20s (- 1m 54s) (34700 69%) 0.0198\n",
      "4m 23s (- 1m 55s) (34800 69%) 0.0196\n",
      "4m 27s (- 1m 55s) (34900 69%) 0.0186\n",
      "4m 30s (- 1m 55s) (35000 70%) 0.0185\n",
      "Val Acc:  0.019632060680538417\n",
      "4m 39s (- 1m 58s) (35100 70%) 0.0189\n",
      "4m 43s (- 1m 59s) (35200 70%) 0.0192\n",
      "4m 46s (- 1m 59s) (35300 70%) 0.0196\n",
      "4m 50s (- 1m 59s) (35400 70%) 0.0182\n",
      "4m 54s (- 2m 0s) (35500 71%) 0.0188\n",
      "4m 57s (- 2m 0s) (35600 71%) 0.0186\n",
      "5m 1s (- 2m 0s) (35700 71%) 0.0189\n",
      "5m 4s (- 2m 0s) (35800 71%) 0.0191\n",
      "5m 8s (- 2m 0s) (35900 71%) 0.0188\n",
      "5m 11s (- 2m 1s) (36000 72%) 0.0196\n",
      "Val Acc:  0.01966184290871024\n",
      "5m 21s (- 2m 3s) (36100 72%) 0.0187\n",
      "5m 25s (- 2m 3s) (36200 72%) 0.0191\n",
      "5m 28s (- 2m 4s) (36300 72%) 0.0190\n",
      "5m 31s (- 2m 4s) (36400 72%) 0.0197\n",
      "5m 35s (- 2m 4s) (36500 73%) 0.0195\n",
      "5m 39s (- 2m 4s) (36600 73%) 0.0187\n",
      "5m 43s (- 2m 4s) (36700 73%) 0.0192\n",
      "5m 47s (- 2m 4s) (36800 73%) 0.0183\n",
      "5m 50s (- 2m 4s) (36900 73%) 0.0176\n",
      "5m 54s (- 2m 4s) (37000 74%) 0.0195\n",
      "Val Acc:  0.019677282221615314\n",
      "6m 3s (- 2m 6s) (37100 74%) 0.0186\n"
     ]
    }
   ],
   "source": [
    "# Initialing hyperparam containers\n",
    "learning_rates = [0.00001]\n",
    "length_regs = [0.0003]\n",
    "continuity_regs = [0.0006]\n",
    "learning_rate_decays = [1]\n",
    "weight_regs = [1e-5, 1e-4]\n",
    "num_epochs = 50\n",
    "\n",
    "load_dict=None\n",
    "for lrate_decay in learning_rate_decays:\n",
    "    for length_reg in length_regs:\n",
    "        for continuity_reg in continuity_regs:\n",
    "            for l_rate in learning_rates:\n",
    "                for wt_reg in weight_regs:\n",
    "                    load_dict = torch.load('test1_softmax_' + str(wt_reg) + '/chkpt_34000True')\n",
    "                    attn_encoder = AttnEncoder.AttnEncoder(pretrained_embeddings, 200, 2, 50, 'LSTM', dropout=0.1)\n",
    "                    attn_encoder.float()\n",
    "\n",
    "                    if (use_cuda):\n",
    "                        attn_encoder.cuda()\n",
    "                    save_folder = 'test1_softmax_' + str(wt_reg) + '/'\n",
    "                    trainIters(X, ratings, X_val, ratings_val, attn_encoder, \n",
    "                                learning_rate=l_rate, learning_rate_decay=lrate_decay, num_epochs=num_epochs, \\\n",
    "                                print_every=100,val_every=1000,load_dict=load_dict, print_grad_every=-1, \\\n",
    "                              save_folder=save_folder,weight_decay=wt_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from clusterAttnWeights import clusterAttnWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPredictions(X, ratings, attn_encoder):\n",
    "\n",
    "    # iterate through X_test and pass to generator->encoder to get mse_error and compare it to truth\n",
    "    num_test_examples = len(X)\n",
    "    X_test_size = num_test_examples\n",
    "    num_iters = X_test_size // (batch_size)\n",
    "    total_loss = 0.0\n",
    "    slice_index = 0\n",
    "    for iters in range(num_iters):\n",
    "\n",
    "        # get X_batch, ratings_batch\n",
    "        # This sampling also preserves the order\n",
    "        \n",
    "        # X_bch = []\n",
    "        # ratings_bch = []\n",
    "        # _ = [ ( X_bch.append(X[i]) , ratings_bch.append(ratings[i]) ) for i in sorted(random.sample(range(num_test_examples), batch_size)) ]\n",
    "\n",
    "        X_bch = X[slice_index : slice_index + batch_size]\n",
    "        ratings_bch = ratings[slice_index : slice_index + batch_size]\n",
    "        # ratings_bch = []\n",
    "\n",
    "        # almost done here - make all the reviews of equal length now\n",
    "\n",
    "        maxlen_rev = max(X_bch, key=len)\n",
    "        maxlen = len(maxlen_rev)\n",
    "\n",
    "        max_seq_len = min(maxlen, MASTER_MAX_VAL_LEN)\n",
    "        \n",
    "        X_bach = np.empty([batch_size,max_seq_len])\n",
    "        ratings_bach = np.empty([batch_size,1])\n",
    "        \n",
    "        encoderLoss = nn.MSELoss(reduce=False)\n",
    "\n",
    "        for iterr in range(batch_size):\n",
    "            currentlen = len(X_bch[iterr])\n",
    "            if (currentlen < max_seq_len):\n",
    "                zero_count = max_seq_len - currentlen\n",
    "                X_bch[iterr].extend([0]*zero_count)\n",
    "            else:\n",
    "                X_bch[iterr] = X_bch[iterr][0:max_seq_len]\n",
    "            # X_bch[iterr] is now a list containing indices of words\n",
    "            # Convert it into a Variable ?\n",
    "            to_append = np.array( X_bch[iterr] )\n",
    "            X_bach[iterr] = to_append\n",
    "            to_append = np.array( ratings_bch[iterr] )\n",
    "            ratings_bach[iterr] = to_append\n",
    "        # X_bach is a 2d numpy array of size :: batch_size X maxlen\n",
    "\n",
    "        if (use_cuda):\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor).cuda()\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor).cuda()\n",
    "        else:\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor)\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor)\n",
    "\n",
    "        X_batch = Variable(X_bach_tensor)\n",
    "        ratings_batch = Variable(ratings_bach_tensor)\n",
    "\n",
    "#         init_hidden = generator.initHidden(batch_size, use_cuda)\n",
    "#         z_sample = generator.sample(X_batch, init_hidden, use_cuda)\n",
    "\n",
    "        attn_weights, ratings_pred = attn_encoder(X_batch, use_cuda, False)\n",
    "        encoder_loss = encoderLoss(ratings_pred, ratings_batch.squeeze(1))\n",
    "\n",
    "        for prnt_ind in range(batch_size):\n",
    "            fin_ind = slice_index + prnt_ind\n",
    "            review_indices = X[fin_ind]\n",
    "            attention_here = attn_weights[prnt_ind]\n",
    "            attn_clip = clusterAttnWeights(attention_here.data.cpu().numpy())\n",
    "            \n",
    "#             sorted_attention = sorted(attention_here.data.cpu().numpy(), reverse = True)\n",
    "#             k = 15\n",
    "#             k_best = sorted_attention[:k]\n",
    "            for indix in range(len(review_indices)): # omitting the attention given to appended indices(to make all reviews of the same length in a batch)\n",
    "                if (review_indices[indix] == dict_str2ind['<EOS>']):\n",
    "                    continue\n",
    "                word = dict_ind2str[review_indices[indix]]\n",
    "                attn = float(attention_here[indix])\n",
    "                if (attn > attn_clip):\n",
    "                    print(\"->\", word, \"<-\", end = ' ')\n",
    "                else:\n",
    "                    print(word, end = ' ')\n",
    "#                 if attn in k_best:\n",
    "#                     print(\"->\", word ,\"<-\" , end = ' ' ) #  -- [\", \"%.4f\" % attn, \"]\"\n",
    "#                 else:\n",
    "#                     print(word, end = ' ')\n",
    "#                 if attn >= 0.05:\n",
    "#                     print(\"->\", word, \"<-\", end = ' ')\n",
    "#                 else:\n",
    "#                     print(word, end = ' ')\n",
    "#                 print(word, \"-- [\", \"%.4f\" % attn, \"]\", end=' ')\n",
    "            print('\\nPred:', float(ratings_pred[prnt_ind]))\n",
    "            print('\\nActual:', float(ratings_batch[prnt_ind]))\n",
    "            print(\"\\n** ** ** ** ** ** ** ** ** **\")\n",
    "            input()\n",
    "            # dict_ind2str \n",
    "\n",
    "        total_loss += float(torch.sum(encoder_loss))\n",
    "\n",
    "        slice_index += batch_size\n",
    "\n",
    "    return total_loss / X_test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shriram/Documents/academics/6th_sem/cs726/project/rationalizing_neural_pred/code/pytorch_venv/lib/python3.5/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/shriram/Documents/academics/6th_sem/cs726/project/rationalizing_neural_pred/code/pytorch_venv/lib/python3.5/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/shriram/Documents/academics/6th_sem/cs726/project/rationalizing_neural_pred/code/pytorch_venv/lib/python3.5/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/shriram/Documents/academics/6th_sem/cs726/project/rationalizing_neural_pred/code/pytorch_venv/lib/python3.5/site-packages/torch/serialization.py:367: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/shriram/Documents/academics/6th_sem/cs726/project/rationalizing_neural_pred/code/project_code/attn_model/AttnEncoder.py:49: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  hidden, (h_n, c_n) = self.lstm_i2h(x_transpose)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the main problem with this beer is that it has no taste , do they use any hops ? it pours a lighter than piss yellow color -> , <- gives off -> little <- -> to <- -> no <- -> aroma <- -> . <- -> apparently <- -> this <- -> is <- -> one <- of the top sold beers in the world , thats -> the <- -> problem <- -> its <- -> way <- too mass produced to have any -> beer <- value -> . <- \n",
      "Pred: 0.313476026058197\n",
      "\n",
      "Actual: 0.30000001192092896\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "it is very unfortunate this situation we have here . i used to love this beer but as my craft beer taste have grown , i have become increasingly less apt to drink a guinness . it has even fell below a fall back beer at a macro tap bar . i am not sure if this is completely related to my tastes or the degrading quality of the guinness product . this nitro bottle is by far the worst serving type i have had . i tired two again last night . one directly from the bottle and one poured into a glass . here is the rundown . out of bottle it is in a bottle to speak of t-bitter chocoalte , black coffee and roasted malt , all very mild flavors m-watery d-low poured into glass a-dark brown , not opaque . not a great cascading head settling -> to <- -> a <- big bubbly head -> , <- -> atypical <- -> of <- -> the <- -> nitro-tap <- -> or <- nitro-can -> s-a <- little -> roasted <- -> grain <- t-same as above m-same as above d same as above \n",
      "Pred: 0.6218199729919434\n",
      "\n",
      "Actual: 0.4000000059604645\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "appearance is a light golden yellow with a thick crisp head and fair -> lace <- -> for <- -> a <- -> malt <- liquor -> . <- -> smell <- is slightly sweet with corn , yeasty with hops . taste has the adjunct flavor to it , a little metallic and stale hops . mouthfeel is is light with a good amount of carbonation . not one i would willingly buy again , it kinda left a light bitter metallic aftertaste i did n't quite care for . \n",
      "Pred: 0.6081474423408508\n",
      "\n",
      "Actual: 0.800000011920929\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "though this beer is , or course , not carbonated , its appearance deserves at least some attention . while it lacks anything remotely resembling a head , its a very elegant looking deep black -> body <- -> with <- -> cherry <- hues and leaves a thin transparent film -> on <- the sides of the glass . if it was carbonated , it would have a 5 for appearance . the smell is of bourbon , vanilla , sweet dark fruits ( especially cherries ) . very pleasant . imperial stouts need not be all that carbonated , but no carbonation whatsoever can not possibly do any beer justice , and this is no exception . the alcohol stands out more so than anything else , which clearly points to the lack of carbonation not being able to release much of the other flavors . there is a slight sweetness up front , but just alchol through the middle and finish , and this is topped off with an unpleasant roasted bitterness ( unpleasant in its obscurity ) . a hint of chocolate noticeable on the palate deep into the after-taste saves the flavor from receiving the fateful `` 1 '' . this is a drain pour less than a quarter of the way through . mouthfeel is extraordinarily light and obviously uncarbonated . sips , drain pour . great job again port/lost abbey . you 've managed to bottle yet another beer with great potential and no carbonation . \n",
      "Pred: 0.8479614853858948\n",
      "\n",
      "Actual: 0.800000011920929\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "vintage 07 batch 3 poured into chimay chalice a- this beer pours a murky brown with a slightly orange tint . alot of yeast floating around with slow steady pour . almost no head -> . <- -> s. <- caramel , rasions , brown sugar , booze and oak . this brew smells heavenly . t. taste goes right along with smell . this is a great tasting beer . wood earthyness , booze soaked fruit , brown sugar is there . taste is amazing . o . this was a very slow drinker for me but well worth the price . for such a high abv it is very drinkable and i will be picking up a few more to cellar . \n",
      "Pred: 0.5860467553138733\n",
      "\n",
      "Actual: 0.6000000238418579\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "aroma : nice grapefruity hoppy aroma , with just a little bit of skunk . sweet malt in the nose as well . appearance : dark amber in color , nice white fluffy head -> , <- ok retention -> , <- -> very <- -> nice <- -> lacing <- -> . <- -> taste <- -> : <- the foretaste gives me a moderate amount of hops , but what i 'm noticing more is the sweet malt backbone of this beer . on subsequent sips , the hope flavor does come through more , but i 'm not sure i 'd agree with the `` mega hopped '' description on the label . the hops are really interesting though , i 'm getting some apricot/peach flavors , along with mint . the mint is what sticks around through the aftertaste . mouthfeel : this beer has a low to medium body with good carbonation , and a delightful smoothness . drinkability : i was expecting more hop bite from this beer , but i ordered it online , so there 's no telling how long it 's been on the shelf . it did still have delicious and unique hop flavors . \n",
      "Pred: 0.8295671343803406\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "got a 12 pack of this for $ 10.99 this beer is a great thirst quencher on a hot summer 's day . i would pick this over a chill ( insert gimmicky -> flavored <- -> beer <- here ) -> any <- day of the week . not an overpowering -> lime <- -> taste <- -> but <- -> still <- much better than a bud -> light <- -> . <- -> good <- -> work <- ab -> . <- \n",
      "Pred: 0.372970312833786\n",
      "\n",
      "Actual: 0.4000000059604645\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "from a bottle , pours black with a large creamy head and thick lacing , very pretty beer . -> nose <- is quite roasty , almost smokey . with a touch of what reminds me of german hops and dark fruits . some toasted nuts . creamy in the mouth , opens with roasted coffee . quite a bit of bittering hops on the finish , again not a typical american use of hops . a bit oily , in its way , almost watery . a nice toasted nut element , good smokey , roasted flavors , but the body is thin and watery to me . pretty nice . \n",
      "Pred: 0.9512932300567627\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "picked this up in for a christmas treat . a : pours a clear golden straw . it is an extremely pale yellow , with only about 1/2 finger of pure white head . the foam settles quickly on this brew . -> s <- -> : <- smells of sour grapes , lemon , cherry , grass , and brett . i actually find the nose of this beer very appealing . t : there is obvious brettanomyces , but i 'm also picking up the scents in the nose . there are flavors of grape , lemon , cherry , green apple , and grass . however , thee is also a really mellow vanilla taste to this beer as well . it 's nice . m : light bodied beer . it starts off tart with medium carbonation . it transitions to smooth in the finish , and the flavors linger for a little bit . the brett is unmistakable . o : i think this beer is really good . i have n't seen this beer hit the shelves in buffalo yet , but if it was there , i 'd pick up a bottle . i 'm really enjoying what anchorage is doing with their bottled beers . \n",
      "Pred: 0.5131127238273621\n",
      "\n",
      "Actual: 0.6000000238418579\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "a thick head rises from the deep brown -> of <- -> this <- -> smoked <- brown -> ale <- -> . <- the off-white -> foam <- -> lingers <- -> for <- a -> while <- and would make a certain -> stout <- from ireland jealous . as it dissipates -> and <- you drink , bands of -> lace <- -> follow <- your progress -> . <- the -> smell <- smacks of malt and you get a slight whiff of smoke . strong roasted malt flavors dominate resulting in a sweet earthiness . the promised smoke does come through ... any more smoke flavor would be overwhelming so the subtlety of this aspect is well-done and appreciated . this beer lingers around a while in your mouth in the back corners of your mouth and the aftertaste is a little bitter . this beer brings a nice experience , but does not compel you to drink one after the other . the slight smokiness does remind me of camping , my favorite part of this beer . \n",
      "Pred: 0.9050383567810059\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "a : this poured a muddy dark mahogany with no head . not the most attractive sight -> . <- -> s <- -> : <- it smelled like strong oak and bourbon and vanilla , with rich chocolate and toffee and port . t : it tasted like rich caramel and toffee and honey wheat , with lots of oak and vanilla and bourbon , and a bit of port and chocolate . it had a smooth and mostly sweet aftertaste . m : it was slightly thick with a little carbonation . decent body . d : this had the perfect combination and balance of flavors for a bourbon ale . very delicious and complex . the body was decent ( could have been fuller ) , and the alcohol was n't very noticeable . so it was pretty easy to sip , but should never be drunk any faster . lots of potential for aging this one . excellent beer . \n",
      "Pred: 0.5296515822410583\n",
      "\n",
      "Actual: 0.6000000238418579\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "i honestly feel like this is one of the best mass-produced adjunct lagers out there ... it is n't crazy great , but not awful , and certainly better than lots of other crap out there . pours a really faint straw colour -> . <- -> the <- head -> is <- about one finger , -> but <- disappears quickly -> . <- smells boring ... -> a <- -> little <- -> grassy <- -> . <- -> not <- much to talk about . the taste is n't all that bad . it starts out with a little hop bitterness and moves into some grainy notes towards the finish . very carbonated , but it lends itself nicely to a medium kind of body . i would n't turn one down , but there are many better beers out there . \n",
      "Pred: 0.34751349687576294\n",
      "\n",
      "Actual: 0.4000000059604645\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "dayton , oh alefest : a very smooth and very drinkable beer that was light formed a small sized tan head with fine sized had a clear and uncarbonated body with a reddish-gold hue.the -> nose <- was malty and mildly hoppy.the flavor was hoppy which left an aftertaste.the lacing was fair with this beer . \n",
      "Pred: 0.5818876624107361\n",
      "\n",
      "Actual: 0.4000000059604645\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "a : this beer poured a copper red brew with a 1 inch head that disappeared very quickly . little to no lacing -> was <- present -> . <- -> s <- -> : <- not a whole lot going on here . subtle smell of ester hops , piney and perhaps again- a subtle hint of perhaps some toasty malt . t : not a bad brew . nutty , toasty and a little hop finish . nothing to get real excited about . clean and crisp . m : thin body . not much going on here at all . d : you could get down with these as a good session beer . it 's very clean , crisp and does carry a good balance . overall- $ 17.00 for a case ; what more could you ask for . not to bad . \n",
      "Pred: 0.5115102529525757\n",
      "\n",
      "Actual: 0.4000000059604645\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "this is an impressive british style ipa . it pours a clear yellowish -> with <- -> plenty <- of white head and -> lots <- of lacing -> . <- -> retention <- -> is <- -> okay <- -> , <- -> not <- great . the -> smell <- is a prodigious mixture of yeast , hops , piney notes , and bready malts . this is a bottle conditioned beer , the carbonation is moderate , and the taste is very raw hops , with astringent grass like characteristics . as the beer warms , there is some more mellowing citrus flavors popping through . this ale is medium bodied and very dry in the nose and mid section . the after taste is a bit overwhelming and for that i lowered the mouthfeel numbers a bit . there is a bit of creaminess on the tongue , but for the most part your getting a pretty substantial british bitter , that is dry , crisply hopped , has loads of bittering aftertastes , and is quite refreshing . l'chaim . \n",
      "Pred: 0.7744431495666504\n",
      "\n",
      "Actual: 0.6000000238418579\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "the appearance is almost black with a quarter inch head that leaves no lacing -> . <- -> the <- -> smell <- is of coffee , roasted malt , very sweet chocolate , and perhaps im getting a little bit of hops in there . the taste starts off with coffee which is followed by the chocolate which is very sweet and is ended by a hop bitterness accompanied by coffee again . the mouthfeel is medium bodied and the drinkability is pretty good , especially for a stout . \n",
      "Pred: 0.6531142592430115\n",
      "\n",
      "Actual: 0.6000000238418579\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "fresh 2010 release beautiful dark amber with orange highlights . sticky and foamy ivory colored head , about 3 fingers thick to start and leaves behind a thick and beautiful lace -> . <- -> sweet <- and robust malty aroma , full of caramel and fruity scents . a slightly alcoholic kick before the hop aromas come in . citrus seems to be the most notable hop aroma . sweet , a bit spicy , and full of bitterness . malty upfront with a warming alcoholic kick . hops make up the finish with lots of citrusy flavors and a nice bitterness to round it all out . mouthfeel is chewy and thick . overall , stone has another quality brew here . lots of great things going on here but it will be interesting to see how this one ages . \n",
      "Pred: 1.0035176277160645\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "dayton , oh alefest : a light bodied beer that was very smooth on the poured , this beer had a large creamy white head -> that <- -> contained <- fine sized -> bubbles.the <- -> body <- was clear -> and <- there was some carbonation noted.the hue was -> straw <- yellow and the -> nose <- was malty and also clove.the flavor was lightly bitter . \n",
      "Pred: 0.7943235039710999\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "thanks to rageagainst123 for this ! pours black -> as <- night . very minimal mocha -> head <- -> , <- produced only by a vigorous pour -> . <- -> lots <- of alcohol coats the sides -> . <- heaps of dark fruits and spicy belgian yeast come through on the aroma . some chocolate and coffee , but the big characters are the former . some alcohol heat in there . tastes fairly similar . lot 's of dark fruits , belgian spiciness , some chocolate and coffee , fairly roasty . not quite full bodied , no carbonation , very slick . a pretty decent stout , but i 'm not sold on the belgian aspect . i think i prefer a good ol ' normal interpretation . \n",
      "Pred: 0.7188272476196289\n",
      "\n",
      "Actual: 0.8999999761581421\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "750ml bottle purchased in chicago , somewhere around $ 10 . the first noteworthy observation is the intense head . there was no gushing from the bottle but each pour generated inches of foam , threatening overspill from the rising suds . the foam itself is dry looking and laces -> in <- thick -> clumps <- -> , <- looks like -> duvel <- -> in <- -> that <- -> regard <- -> . <- the liquid is hazy and -> pale <- golden colored . there 's a lot in the aroma , with honey and brettanomyces being the dominant themes . a little yeastiness in the background , reminiscent of st bernardus tripel . this beer tastes very good , exceptionally good , lemon , spice , fermented honey , and just a little bret character giving some wild notes . dry finish , only slightly bitter . light carbonation which was odd with the thick foam , i guess everything it had went into the head . very good and completely recommended , just be careful with the pours . \n",
      "Pred: 0.9353269338607788\n",
      "\n",
      "Actual: 0.6000000238418579\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "a special thanks to trumick for this one . hard to get . a- small 6 oz bottle , 2007 , pours dark brown/red , small head , thin bubble lacing -> . <- -> s <- -i have never had anything like it . the raspberries dominate the nose , with chocolate not far behind . t- taste amplifies the smell . it is like eating a chocolate torte smothered in raspberry sauce . m - very coating , creamy , sufficient carbonation . d - i like dessert . i wish i had a case of these guys to finish off my next 24 meals . \n",
      "Pred: 0.5829460620880127\n",
      "\n",
      "Actual: 0.699999988079071\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "very tangy- you can taste the citrus in it . the rest is almost indescribable . there is something very drinkable about this beer as it is quite sweet , and it is easy to quaff it down by the case load . a good starting beer , -> but <- like most i 've had there are n't really any complex flavours to it . very unique though , as the -> flavourings <- are the same that are used to flavour tequila . the more of this i drink , the worse it seems to get . it has an odd aftertaste ... but never fear ! add some lime or lime -> juice <- as you would tequila , clears up the aftertaste and finishes it off well . \n",
      "Pred: 0.5219420194625854\n",
      "\n",
      "Actual: 0.4000000059604645\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "my brother picked this one up and i was quite surprised with the results . a - pours a nice golden brown with an amber red hue , two thumbs of bubbly beige head and retention properties which are everlasting . lacing was intense leaving mountains of bubbles stuck to the sides of the glass -> . <- -> s <- -> - <- the sweet malts come through strong with chocolate , brown sugar , toffee , caramel and honey all noticeable on the nose . some spicing was also noticeable with some cloves and coriander fighting their way through the bouquet of scents . t - this is a delectable dark belgian , with coffee and chocolate coming through nicely , complemented by cloves on the finish . the higher abv is detectable , but not offensive . the alcohol becomes more prominent as it warms , but again , it does n't detract from the overall tastiness of this drink . m - perfectly balanced on carbonation , goes down smooth like a stout , but with a little extra carbonation to give it a lift . d - blown away at how well this one hides the abv . as previously mentioned , it does become more noticeable as it warms , but it is not offensive . i could actually have more than one of these in a single session , though that could end up being rather disastrous . highly recommended to anyone looking for a dark , malty belgian ! ! \n",
      "Pred: 0.9466003179550171\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "chilled bomber into a glass . a : cloudy lemon / straw color with active carbonation up the middle . huge fluffy white head that lingers and lingers , leaving thick lacing down -> the <- -> glass <- -> . <- -> s <- -> : <- -> sour <- apples , orange/citrus , some grass , cloves , and coriander . a characteristic smell of lighter unibroue offerings . t : apples at the front , with hints of peach and banana . some grass and weizen bitterness in the middle . a sweet champagne-like finish . no alcohol detected . m : slightly thin-bodied , or at least feels so due to the generous carbonation . too much bubbly ; this would stand to benefit from being a bit crisper . d : decent , but not great , unibroue offering . they have other stuff i prefer more . i enjoyed this but i wo n't seek out another . \n",
      "Pred: 0.9291390180587769\n",
      "\n",
      "Actual: 0.8999999761581421\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "first off i should say i am a huge fan of dogfish head brews -> . <- a= this beer 's weakest point , bar none , is its appearance -> . <- it pours a rather clear pale amber color -> with <- -> absolutely <- no head -> , <- -> even <- -> with <- -> an <- aggressive pour -> . <- -> s= <- -> this <- -> beer <- -> smells <- -> clean <- -> and <- -> sweet <- -> . <- most of the smells come from the funky combination of the raisins and the yeast . t= this is by no means a bad beer , but it sort of reminds me , in its appearance and taste , of newcastle brown ale with raisin sweetness added . it sports a modest malt flavor , low hopping , and of course that raisin sweetness . mf= some crispness in the mouth , with residual sugars forming a bit of body . d= while i can not say that this is my favorite beer in the world , it sure goes down deceptively easy for such a high abv brew ( 8 % ) . this beer is a good choice for those who do n't like to be `` challenged '' by the taste of other high abv brews such as certain belgian ales , barley wines , etc . \n",
      "Pred: 0.518626868724823\n",
      "\n",
      "Actual: 0.5\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "have n't seen ommegang beers in very many places around my area until just the past few months . interestingly enough i stopped in a random dandy mini mart ( typical store ) and as i was waiting to pay for my gas at the front i looked to the back of the store . to what to my surprise did i see , but yes , an ommegang display rack . this surprised me because this place has always been your typical place to find deals on such things as natty light and busch . i do n't know if it is selling in this locale , but hey , god bless them for trying . point is their beer is starting to pop-up everywhere in the southern tier not just in the better beer stores , and that ca n't be a bad thing . anyway , about the beer . although it was n't my first time with a hennepin saison and certainly not the last , it is the first time i 've reviewed it . i purchased a bomber at the store around the corner from my home and chilled it for a couple of hours before consuming . at first pour the frothy pure white head was abundant and survived throughout the drinking of the beer covering -> the <- -> hazy <- -> straw <- color -> of <- -> the <- liquid below . as for -> smell <- , i do n't have the greatest of ability in this area , but to be sure there was nothing in the slightest bit `` off `` about this beer , and it was at least pleasant in that respect . the taste had a very light sweetness to it and a definite clove , corriander , citrus undertone . although it has a 7.5 abv it seemed much lighter in mouthfeel , something a `` '' ( like i can sometimes be ) should be wary of on a hot summer afternoon . i definitely see myself buying this beer more often now that it is readily available . it is a great summer beer that is light and refreshing . and at $ 4.99 retail , not a bad price for a very high quality product . \n",
      "Pred: 0.8015590310096741\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "a : cola colored . -> clean <- with burgundy highlights -> . <- poor -> carbonation <- -> , <- -> very <- fizzy -> ( <- like cola ) -> with <- a rapidly depleting head -> . <- -> s <- -> : <- -> blackstrap <- -> molasses <- , leather , honey and teriyaki . t : lots of syrupy dark grains and molasses up the ying yang . some oaky flavors , raisin and dried plum with some licorice mixed in at the finish . almost salty m : very thick and chewy , almost sticky . could use some more carbonation . d : extremely thick and sweet , but cloying and overbearing . very good bock style beer -> . <- \n",
      "Pred: 0.524137556552887\n",
      "\n",
      "Actual: 0.5\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "the recipient , made with half a coconut is quite , not to say more . otherwise the drink is whitish , very milky and -> assertive <- flavors of -> coconut <- -> . <- if the entry is first on the palate silky and light coconut -> milk <- arrives soon and it looks so -> real <- you can smell the -> walnut <- -> pieces <- -> . <- not really bad but still coarse mixture between coconut and a free beer or milk taste inevitably takes the advantage , but it may be interesting , at least on a small scale , for fans of the fruit . \n",
      "Pred: 0.649082601070404\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "oh , harpers , so much enthusiasm , so little wow ! . this seems to be a limited offering , and i think i know why . pours dark black with a tawny brown head , actually looks pretty -> nice <- -> . <- -> smells <- really sweet and roasty , but somehow acidic and just plain `` off . '' taste is ok , but can be hard to take . the sweetness of the brew is offset by the bitter roast quality , which actually overwhelms . smooth mouth , but otherwise unimpressive . not very drinkable . considering the strength and general sweetness of the brew , not recommended to have more than one . this could have been very nice , but turned out over-the-top and like homebrew gone bad . \n",
      "Pred: 0.8471914529800415\n",
      "\n",
      "Actual: 0.8999999761581421\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "a- beautiful golden amber pour -> , <- -> with <- creamy head -> much <- like a -> medium-colored <- guinness -> s- <- caramel , malts with a diacetyl and bittersweet undertone t- rich butterscotch with little to no sweetness m- slick yet smooth , low carbonation d- went down like water ... much better st. paddy 's choice than the other one you see everywhere \n",
      "Pred: 0.9355604648590088\n",
      "\n",
      "Actual: 0.8999999761581421\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "a-dark amber with a thick cream colored head -> . <- -> decent <- lacing -> s-smells <- -> spicy <- , herbal , and hoppy . good strength t-comes in with a smooth taste of stale grass , herbs , spices , and sweet hops . its not a great taste but its a high quality taste that works . there 's a bitterness that lasts in to the after-taste of grass , grains , and herbs . m-creamy medium carbonation with a crisp finish ipas are n't my favorite , but this is quite tasty . its definitely a worthwhile ipa . it seems to get more complex as the evening goes on . \n",
      "Pred: 0.8399273157119751\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "draught pint at the brewpub ( fenway ) on 2/8/06 . perfect pour in all aspects . the head is large , yet dense and lasting -> , <- a burnt sienna color -> , <- -> consisting <- of very minute bubbles . lacing is created in sheets down the -> glass <- , while the beers color is near black -> , <- certainly opaque -> . <- the -> aroma <- is a light espresso , with bits of dark chocolate , tons of black malt and/or roasted unmalted barley . very earthy , dry , dripping with unsweetned molasses . warming allows some light creamy-sweet malt flavors to emerge in the nose , but an increasing amount of hop bitterness also begins to appear in the nose , battling back any sweetness to be seen . the flavor shows the strong black malt/roasted barley character of the nose , and runs all over the palate with it . intense , black , old coffee , high alpha acid hop bitterness and biting acidity from the roasted barley as well . it dosent start out over the top ( and its served extremely cold to start ) , but the more it warms , the more the acidity increases . at first its rather masked and there is some lightly sweet chocolate and graham cracker going on , but its short-lived . alcohol provides warming , though it is not too pervasive . the body is a bit watery , which also contributes to the acidity problem . more thick malts might pick that up and dull it better . low carbonation , though , with an oily feel to it . \n",
      "Pred: 0.9356797933578491\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "macro -> lager <- -> . <- what do you think it looks like ? nose is incredibly sweet , in that insipid ( can a smell be insipid ? ) caramel malt ( with a healthy dollop of rice/corn , i 'm sure ) candy that this kind of beer often has . flavor is sickly sweet as well - reminds me of rolling rock . fortunately , there 's a ton of carbonation in here , so it masks the flavor fairly well . overall , this beer is bad . a buddy told me that the brewery is a hot mess so every beer tastes different . but i tried two and they both sucked , so forget this . \n",
      "Pred: 0.35590022802352905\n",
      "\n",
      "Actual: 0.4000000059604645\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "from notes on 7/18/11 at kalamazoo beer exchange 85 ibu they call it a `` black ipa '' . a : pitch-black beer . light brown head that pours about a half finger , but stays about a half a finger nail the whole way down . with a good amount of lacing -> . <- -> s <- -> : <- dry hops , deep , dark , smoky malt t : coffee , bourbon , and chocolate . along with the classic hoppy-ness of an ipa . a bitter and hoppy ( but not overly hoppy ) finish . m : medium to full body with low carbonation . o : pretty filling . i feel the same way as a feel about hefeweizen i was i had a proper environment to rate this beer . its different and delicious . \n",
      "Pred: 0.9150100350379944\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "hook and ladder backdraft brown ale is a nice , deep red-brown ale that 's very clear and dense in appearance . it 's lightly sparkling with a thin , dense , splotchy beige head that leaves nice lacing . a beautiful beer -> ! <- -> it <- -> has <- -> a <- fresh aroma of chocolate and grapefruit . it has a smooth , medium body with a dry , slightly chalky , finish . it has good balance in flavor . there is a slight touch of sweet caramel and a decent amount of bitterness . i could have more than one of these . what a pleasant surprise ! i had the impression that this was brewed at high falls brewery in rochester , ny . ( rated aug 5 , 2008 ) \n",
      "Pred: 0.9641903638839722\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "if there was such a thing as yellow mineral water -> , <- i would assume that this is what it would taste like -> . <- pours a -> light <- -> straw <- colour -> . <- -> faint <- -> skunky <- -> aroma <- -> that <- -> is <- -> a <- -> precursor <- -> to <- -> bland <- -> beer <- crap -> . <- why crap ? cause theres no taste . crap . stay away from this cheap beer -> . <- cold filtering must mean it takes out all the taste leaving your tastebuds in the cold -> . <- ugh -> . <- \n",
      "Pred: 0.26617807149887085\n",
      "\n",
      "Actual: 0.20000000298023224\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "this very pale yellow -> brew <- -> is <- topped -> by <- -> a <- -> medium <- thick head -> with <- -> some <- lacing -> left <- -> behind <- -> . <- -> it <- -> has <- a -> sweet <- -> fruity <- -> aroma <- with a touch of hops . the flavor is well balanced and slightly sweet with malt . the flavor quickly turns hoppy with a slightly dry bitter hoppy finish . this beer is very smooth and light . \n",
      "Pred: 0.5303109288215637\n",
      "\n",
      "Actual: 0.4000000059604645\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "12oz from bucket for about $ 3.75 ( i think ) .. shared grudgingly with brenda.. the lion 's share from my new belgium goblet . one finger light tan head dies fairly quickly leaving -> behind <- a -> suprising <- amount of -> lacing <- -> ... <- -> deeep <- -> brown <- -> brew <- -> ... <- only reveals ruby highlites when held up to a bare bulb -> . <- dark bread and subtle molasses are the name of the game.. some dark caramel ... only a touch of dried fruit comes through.. figs i guess ... as it warms it reminds me of a cold mocha coffee without the sugars . flavor is complex dark malts.. and a waft of raisin and a little more dark caramel ... light but present nuttiness ... not alot of chocolate or coffee , just a touch . for me , the complexity lies in the dark malts.. very tasty ... subtle noble hops are easily overlooked ... i always walk into celebrator remembering a bigger body , but the body is really just a solid medium with a creamy carbonation and mouthfeel ... with warmth comes a touch of peatiness . i know that what i am enjoying is world-class , although my simplistic palate is not refined enough to really describe it.. \n",
      "Pred: 0.7272396683692932\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "water . water . water . but damn , it 's water that gets you drunk . natty light was my high school drink of choice , so i continue the tradition today . it really has no flavor to speak of , but perhaps that 's a good thing -> . <- -> unlike <- beast or , it doesnt make me want to vomit with one sip . in any case - it doesnt taste like beer , but it aint bad if you need to get a quick load . try a funnel or shotgun technique . whatever . \n",
      "Pred: 0.31387415528297424\n",
      "\n",
      "Actual: 0.20000000298023224\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "2006 vintage . thanks to for bringing this back for me . a - no head , -> dark <- -> body <- -> . <- eh -> . <- -> s <- -> - <- nice sweet aroma , chocolate , wood and coffee notes , all very well balenced . slight bitterness at the finish . no off aromas . t - starts out to tart for my tastes , but finishes very nice and dry . noted cinnamon , chocolate , but lots dead points in the palate . i expected a lot more ... m - weak and lacking carbonation ... d - i feel like a lot of the problems with with beer come from the fact that has just been aged to long . would love to try a fresh batch sometime ... \n",
      "Pred: 0.46637511253356934\n",
      "\n",
      "Actual: 0.4000000059604645\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "this was over -> hopped <- to my palate and had a strange aftertaste of bitterness that was not enjoyable . the normal bitter -> refreshing <- characteristics are gone and the overly bitter exotic tastes come through in a way you do not want to drink them . this made urquell 's taste seem amazingly more refreshing and enjoyable . \n",
      "Pred: 0.6682018041610718\n",
      "\n",
      "Actual: 0.800000011920929\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n",
      "1\n",
      "i finally tried this brew . should 've tried this sooner ! a - brilliantly clear golden amber color with a huge billowy white head -> . <- rivals the clarity i 've seen -> from <- -> some <- very highly rated micro 's ! s -> - <- wow , dry hopped but very mild aroma - floral and citricy hops , bready malts . t - very nice citric / floral hop effect on a biscuit malt base . very well balanced but less sharp on the hop note than expected . m - medium bodied , nicely carbonated , crisp and smooth finish . d - very very drinkable ! i could see sharing a 12 pack with friends and everyone being quite happy with the choice . \n",
      "Pred: 0.983293890953064\n",
      "\n",
      "Actual: 1.0\n",
      "\n",
      "** ** ** ** ** ** ** ** ** **\n"
     ]
    }
   ],
   "source": [
    "attn_encoder = torch.load('clean_run/test_softmax_0.0274_3e-06/chkpt_17000True', map_location={'cuda:1':'cuda:0'})['attn_encoder_model']\n",
    "# attn_encoder.cuda(device='0')\n",
    "getPredictions(X, ratings, attn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
