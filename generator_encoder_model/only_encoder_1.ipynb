{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import random\n",
    "\n",
    "import Generator\n",
    "import Encoder\n",
    "import TempEncoder\n",
    "# Global defs\n",
    "\n",
    "# iters_per_epoch should also be shifted here ?\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "batch_size = 64\n",
    "\n",
    "MASTER_MAX_LEN = 300\n",
    "MASTER_MAX_VAL_LEN = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function defs\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def asMinutes(s):\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "\tnow = time.time()\n",
    "\ts = now - since\n",
    "\tes = s / (percent + 1e-8)\n",
    "\trs = es - s\n",
    "\treturn '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ** Starting of the Main code \n",
    "# ** ** \n",
    "# ** ** \n",
    "import re\n",
    "\n",
    "# Convert string to vector of floats\n",
    "def convert_to_float(string): # string with float values separated by spaces\n",
    "\tlis = string.split()\n",
    "\tlis_rating = [ float(value) for value in lis]\n",
    "\treturn lis_rating\n",
    "\n",
    "# Unique index for words\n",
    "index = 0\n",
    "def get_index():\n",
    "\tglobal index\n",
    "\tto_ret = index\n",
    "\tindex += 1\n",
    "\treturn to_ret\n",
    "\n",
    "# Dictionaries\n",
    "dict_ind2vec = {}\n",
    "dict_ind2str = {}\n",
    "dict_str2ind = {}\n",
    "\n",
    "def get_list_of_indices(string):\n",
    "\tlis_words = string.split()\n",
    "\t# lis_ret = [ for word in lis_words]\n",
    "\tlis_ret = []\n",
    "\tfor word in lis_words:\n",
    "\t\ttry:\n",
    "\t\t\tind_append = dict_str2ind[word]\n",
    "\t\t\tlis_ret.append(ind_append)\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\t\t\t# ind_append = \n",
    "\t\t\t# print(\"THERE IT IS!\", word)\n",
    "\t# print(\"About to return\")\n",
    "\treturn lis_ret\n",
    "\n",
    "## **\n",
    "## **\n",
    "# read the word2vec representations\n",
    "\n",
    "with open('../review+wiki.filtered.200.txt') as f:\n",
    "\twordvecs = f.readlines()\n",
    "\n",
    "first_pair = wordvecs[0].split(\" \", 1)\n",
    "first_vec = convert_to_float(first_pair[1])\n",
    "dim_vecSpace = len(first_vec) # Dimension of the vector space in which we are\n",
    "\n",
    "# add stuff for EOS, Blank\n",
    "# at index = 0, 1\n",
    "\n",
    "eos_index = get_index()\n",
    "dict_str2ind[\"<EOS>\"] = eos_index\n",
    "dict_ind2str[eos_index] = \"<EOS>\"\n",
    "dict_ind2vec[eos_index] = [1.0]*dim_vecSpace\n",
    "\n",
    "blk_index = get_index()\n",
    "dict_str2ind[\"<BLANK>\"] = blk_index\n",
    "dict_ind2str[blk_index] = \"<BLANK>\"\n",
    "dict_ind2vec[blk_index] = [0.0]*dim_vecSpace\n",
    "\n",
    "\n",
    "for elem in wordvecs:\n",
    "\tliss = elem.split(\" \", 1) # split on the first space\n",
    "\tword_str = liss[0]\n",
    "\tword_vec = convert_to_float(liss[1])\n",
    "\t\n",
    "\there_index = get_index()\n",
    "\tdict_str2ind[word_str] = here_index\n",
    "\tdict_ind2str[here_index] = word_str\n",
    "\tdict_ind2vec[here_index] = word_vec\n",
    "\n",
    "# CHKING\n",
    "# print( dict_str2ind['a'] )\n",
    "\n",
    "## **\n",
    "## **\n",
    "# read the data\n",
    "\n",
    "with open('../reviews.aspect0.train.txt') as f:\n",
    "\ttrain_data = f.readlines()\n",
    "\n",
    "rating_regex = re.compile('\\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d\\t') # Exactly matches only the ratings\n",
    "\n",
    "# extract ratings - # each rating is a scalar value # NO ::: each rating is a list of 5 values\n",
    "ratings = [ float( re.findall(rating_regex, review)[0][0] ) for review in train_data ]\n",
    "\n",
    "# extract reviews\n",
    "reviews_str = [ rating_regex.sub('', review) for review in train_data ]\n",
    "reviews = [ get_list_of_indices( review_str ) for review_str in reviews_str ]\n",
    "X = reviews\n",
    "total_size = len(X)\n",
    "\n",
    "divide_train = int( (4*total_size)/5 )\n",
    "train_indices_of_X = sorted( random.sample( range(total_size), divide_train ) )\n",
    "\n",
    "X_train = []\n",
    "X_val = []\n",
    "ratings_train = []\n",
    "ratings_val = []\n",
    "for i in range(total_size):\n",
    "\tif i in train_indices_of_X:\n",
    "\t\tX_train.append(X[i])\n",
    "\t\tratings_train.append(ratings[i])\n",
    "\telse:\n",
    "\t\tX_val.append(X[i])\n",
    "\t\tratings_val.append(ratings[i])\n",
    "\n",
    "X = X_train\n",
    "ratings = ratings_train\n",
    "\n",
    "num_train_examples = len(X) # we also assume len(X) = len(ratings)\n",
    "num_val_examples = len(X_val)\n",
    "# read validation data\n",
    "\n",
    "# ** ** \n",
    "# ** ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAccuracy(X, ratings, encoder, generator):\n",
    "\n",
    "    # iterate through X_val and pass to generator->encoder to get mse_error and compare it to truth\n",
    "    num_val_examples = len(X)\n",
    "    X_val_size = num_val_examples\n",
    "    num_iters = X_val_size // (batch_size)\n",
    "    total_loss = 0.0\n",
    "    for iters in range(num_iters):\n",
    "\n",
    "        # get X_batch, ratings_batch\n",
    "        # This sampling also preserves the order\n",
    "        X_bch = []\n",
    "        ratings_bch = []\n",
    "\n",
    "        _ = [ ( X_bch.append(X[i]) , ratings_bch.append(ratings[i]) ) for i in sorted(random.sample(range(num_val_examples), batch_size)) ]\n",
    "\n",
    "        # almost done here - make all the reviews of equal length now\n",
    "\n",
    "        maxlen_rev = max(X_bch, key=len)\n",
    "        maxlen = len(maxlen_rev)\n",
    "\n",
    "        max_seq_len = min(maxlen, MASTER_MAX_VAL_LEN)\n",
    "        \n",
    "        \n",
    "        X_bach = np.empty([batch_size,max_seq_len])\n",
    "        ratings_bach = np.empty([batch_size,1])\n",
    "\n",
    "        for iterr in range(batch_size):\n",
    "            currentlen = len(X_bch[iterr])\n",
    "            if (currentlen < max_seq_len):\n",
    "                zero_count = max_seq_len - currentlen\n",
    "                X_bch[iterr].extend([0]*zero_count)\n",
    "            else:\n",
    "                X_bch[iterr] = X_bch[iterr][0:max_seq_len]\n",
    "            # X_bch[iterr] is now a list containing indices of words\n",
    "            # Convert it into a Variable ?\n",
    "            to_append = np.array( X_bch[iterr] )\n",
    "            X_bach[iterr] = to_append\n",
    "            to_append = np.array( ratings_bch[iterr] )\n",
    "            ratings_bach[iterr] = to_append\n",
    "        # X_bach is a 2d numpy array of size :: batch_size X maxlen\n",
    "\n",
    "        if (use_cuda):\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor).cuda()\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor).cuda()\n",
    "        else:\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor)\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor)\n",
    "\n",
    "        X_batch = Variable(X_bach_tensor)\n",
    "        ratings_batch = Variable(ratings_bach_tensor)\n",
    "\n",
    "#         init_hidden = generator.initHidden(batch_size, use_cuda)\n",
    "#         z_sample = generator.sample(X_batch, init_hidden, use_cuda)\n",
    "\n",
    "        ratings_pred = encoder(X_batch, None, False)\n",
    "        encoderLoss = nn.MSELoss(reduce=False)\n",
    "        encoder_loss = encoderLoss(ratings_pred, ratings_batch.squeeze(1))\n",
    "\n",
    "        total_loss += float(torch.sum(encoder_loss))\n",
    "\n",
    "    return total_loss / X_val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train function - here's some ingenuity\n",
    "# one iteration of training\n",
    "def train(X, ratings, encoder, generator, encoder_optimizer, generator_optimizer, \\\n",
    "          length_reg, continuity_reg, print_grad_norm):\n",
    "    # X - single batch\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    generator_optimizer.zero_grad()\n",
    "\n",
    "    encoderLoss = nn.MSELoss(reduce=False)\n",
    "\n",
    "    mean_cost = 0.0\n",
    "    mean_encoder_cost = 0.0\n",
    "    for i in range(num_samples):\n",
    "#         init_hidden = generator.initHidden(batch_size, use_cuda)\n",
    "#         z_sample = generator.sample(X, init_hidden, use_cuda)\n",
    "#         z_sample = Variable(torch.zeros((batch_size, X.shape[1])))\n",
    "#         z_sample = z_sample.detach()\n",
    "\n",
    "        ratings_pred = encoder(X, None)\n",
    "        encoder_loss = encoderLoss(ratings_pred, ratings.squeeze(1))\n",
    "\n",
    "#         init_hidden = generator.initHidden(batch_size, use_cuda)\n",
    "#         length_cost, continuity_cost = generator.loss(z_sample)\n",
    "\n",
    "#         cost = encoder_loss + length_cost * length_reg + continuity_cost * continuity_reg\n",
    "        cost = encoder_loss\n",
    "       \n",
    "\n",
    "#         log_prob = generator.logProb(X, z_sample, init_hidden, use_cuda)\n",
    "\n",
    "#         log_prob.backward(1.0 / (num_samples * batch_size) * cost)\n",
    "#         cost.backward(torch.Tensor([1.0 / (batch_size * num_samples)] * batch_size))\n",
    "        cost1 = torch.mean(cost)\n",
    "    \n",
    "        mean_cost += float(cost1)\n",
    "        mean_encoder_cost += float(torch.mean(encoder_loss))\n",
    "        \n",
    "        cost1 /= num_samples\n",
    "        cost1.backward()\n",
    "        \n",
    "        \n",
    "    if (print_grad_norm):\n",
    "        for name, param in encoder.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "                print(param.data.norm())\n",
    "                print (param.grad.data.norm())\n",
    "                \n",
    "        input()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "#     generator_optimizer.step()\n",
    "    \n",
    "    \n",
    "    mean_cost /= num_samples\n",
    "    mean_encoder_cost /= num_samples\n",
    "    return mean_cost, mean_encoder_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainIters(X, ratings, X_val, ratings_val, encoder, generator, learning_rate, learning_rate_decay, num_epochs, \\\n",
    "               length_reg, continuity_reg, load_dict=None, print_every=1000, plot_every=100, val_every=1000, print_grad_every=-1, \\\n",
    "                save_folder=''):\n",
    "\n",
    "    \n",
    "    num_train_examples = len(X)\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0.0\n",
    "    print_encoder_loss_total = 0.0\n",
    "    plot_loss_total = 0.0\n",
    "    best_val_acc = float(\"inf\")\n",
    "    if load_dict is not None:\n",
    "        encoder = load_dict['encoder_model']\n",
    "        generator = load_dict['generator_model']\n",
    "        \n",
    "        cur_tot_iters = load_dict['tot_iter']\n",
    "    else:\n",
    "        cur_tot_iters = 0\n",
    "        \n",
    "    enc_param_list = []\n",
    "    for param in encoder.parameters():\n",
    "        if (param.requires_grad):\n",
    "            enc_param_list.append(param)\n",
    "    encoder_optimizer = optim.Adam(enc_param_list, lr=learning_rate)\n",
    "\n",
    "    gen_param_list = []\n",
    "    for param in generator.parameters():\n",
    "        if (param.requires_grad):\n",
    "            gen_param_list.append(param)\n",
    "    generator_optimizer = optim.Adam(gen_param_list, lr=learning_rate)\n",
    "    \n",
    "        \n",
    "    encoder_scheduler = optim.lr_scheduler.StepLR(encoder_optimizer, 1, learning_rate_decay)\n",
    "    generator_scheduler = optim.lr_scheduler.StepLR(generator_optimizer, 1, learning_rate_decay)\n",
    "    \n",
    "    if (load_dict is not None):\n",
    "        encoder_optimizer.load_state_dict(load_dict['encoder_optimizer'])\n",
    "#         for param_group in encoder_optimizer.param_groups:\n",
    "#             param_group['lr'] = learning_rate\n",
    "        generator_optimizer.load_state_dict(load_dict['generator_optimizer'])\n",
    "        encoder_scheduler.load_state_dict(load_dict['encoder_scheduler'])\n",
    "        generator_scheduler.load_state_dict(load_dict['generator_scheduler'])\n",
    "\n",
    "#     print(encoder_optimizer.param_groups[0]['lr'])\n",
    "     # set iters_per_epoch\n",
    "    iters_per_epoch = num_train_examples // batch_size\n",
    "    n_iters = iters_per_epoch * num_epochs\n",
    "    \n",
    "    position_set = False\n",
    "    for epoch in range(num_epochs):\n",
    "        if (position_set):\n",
    "            encoder_scheduler.step()\n",
    "            generator_scheduler.step()\n",
    "        for iter_num in range(iters_per_epoch):\n",
    "            if (cur_tot_iters >= epoch * iters_per_epoch + iter_num + 1):\n",
    "                continue\n",
    "            \n",
    "            position_set = True\n",
    "            # randomly choose sample from X and make them equal length\n",
    "            # This sampling also preserves the order\n",
    "            X_bch = []\n",
    "            ratings_bch = []\n",
    "\n",
    "            _ = [ ( X_bch.append(X[i]) , ratings_bch.append(ratings[i]) ) for i in sorted(random.sample(range(num_train_examples), batch_size)) ]\n",
    "\n",
    "            # almost done here - make all the reviews of equal length now\n",
    "\n",
    "            maxlen_rev = max(X_bch, key=len)\n",
    "            maxlen = len(maxlen_rev)\n",
    "\n",
    "            max_seq_len = min(maxlen, MASTER_MAX_LEN)\n",
    "            \n",
    "            X_bach = np.empty([batch_size,max_seq_len])\n",
    "            ratings_bach = np.empty([batch_size,1])\n",
    "\n",
    "            for iterr in range(batch_size):\n",
    "                currentlen = len(X_bch[iterr])\n",
    "                if (currentlen < max_seq_len):\n",
    "                    zero_count = max_seq_len - currentlen\n",
    "                    X_bch[iterr].extend([0]*zero_count)\n",
    "                else:\n",
    "                    X_bch[iterr] = X_bch[iterr][0:max_seq_len]\n",
    "                # X_bch[iterr] is now a list containing indices of words\n",
    "                # Convert it into a Variable ?\n",
    "                to_append = np.array( X_bch[iterr] )\n",
    "    # \t\t\t\tX_bach = np.append(X_bach, [to_append], axis = 0)\n",
    "                X_bach[iterr] = to_append\n",
    "                to_append = np.array( ratings_bch[iterr] )\n",
    "    # \t\t\t\tratings_bach = np.append(ratings_bach, to_append, axis = 0)\n",
    "                ratings_bach[iterr] = to_append\n",
    "            # X_bach is a 2d numpy array of size :: batch_size X maxlen\n",
    "\n",
    "            if (use_cuda):\n",
    "                X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor).cuda()\n",
    "                ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor).cuda()\n",
    "            else:\n",
    "                X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor)\n",
    "                ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor)\n",
    "            X_batch = Variable(X_bach_tensor)\n",
    "            ratings_batch = Variable(ratings_bach_tensor)\n",
    "            # call train with this batch\n",
    "            cur_tot_iters = iter_num + 1 + epoch * iters_per_epoch\n",
    "            if (print_grad_every > 0 and cur_tot_iters % print_grad_every == 0):\n",
    "                cur_loss, cur_encoder_loss = train(X_batch, ratings_batch, encoder, generator, \\\n",
    "                                                   encoder_optimizer, generator_optimizer, length_reg, continuity_reg, True)\n",
    "            else:\n",
    "                cur_loss, cur_encoder_loss = train(X_batch, ratings_batch, encoder, generator, \\\n",
    "                                               encoder_optimizer, generator_optimizer, length_reg, continuity_reg, False)\n",
    "            \n",
    "            print_loss_total += cur_loss\n",
    "            print_encoder_loss_total += cur_encoder_loss\n",
    "            plot_loss_total += cur_loss\n",
    "\n",
    "            \n",
    "          \n",
    "            if (cur_tot_iters) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_encoder_loss_avg = print_encoder_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print_encoder_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f %.4f' % (timeSince(start, 1.0 * (cur_tot_iters) / n_iters),\n",
    "                                             cur_tot_iters, 1.0 * (cur_tot_iters) / n_iters * 100, print_loss_avg, print_encoder_loss_avg),flush=True)\n",
    "            \n",
    "            if (cur_tot_iters) % val_every == 0:\n",
    "                val_acc = getAccuracy(X_val, ratings_val, encoder, generator)\n",
    "                print(\"Val Acc: \", val_acc)\n",
    "                if (val_acc < best_val_acc):\n",
    "                    best_val_acc = val_acc\n",
    "                    best = True\n",
    "                else:\n",
    "                    best = False\n",
    "                    \n",
    "                save_dict = {}\n",
    "                save_dict['encoder_model'] = encoder\n",
    "                save_dict['generator_model'] = generator\n",
    "                save_dict['encoder_optimizer'] = encoder_optimizer.state_dict()\n",
    "                save_dict['generator_optimizer'] = generator_optimizer.state_dict()\n",
    "                save_dict['encoder_scheduler'] = encoder_scheduler.state_dict()\n",
    "                save_dict['generator_scheduler'] = generator_scheduler.state_dict()\n",
    "                \n",
    "                save_dict['tot_iter'] = cur_tot_iters\n",
    "                save_dict['val_acc'] = val_acc\n",
    "                save_dict['best_so_far'] = best\n",
    "                torch.save(save_dict, save_folder+'chkpt_'+str(cur_tot_iters)+str(best))\n",
    "\n",
    "            if iter_num % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining pretrained_embeddings\n",
    "pretrained_embeddings = np.empty([len(dict_ind2vec), dim_vecSpace])\n",
    "for key in sorted(dict_ind2vec.keys()):\n",
    "    vec_here = dict_ind2vec[key]\n",
    "    pretrained_embeddings[key] = np.array(vec_here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_dict = torch.load('only_enc2/chkpt_50000False')\n",
    "# encoder = None\n",
    "# generator = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu1604/temp/code/project_code/enc_gen_model/Encoder.py:42: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  output, (_, _) = self.lstm_i2h(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 5s (- 0m 1s) (50100 83%) 0.0418 0.0418\n",
      "Val Acc:  0.06422075743507594\n",
      "0m 19s (- 0m 3s) (50200 83%) 0.0391 0.0391\n",
      "Val Acc:  0.0649057505428791\n",
      "0m 32s (- 0m 6s) (50300 83%) 0.0432 0.0432\n",
      "Val Acc:  0.06840119893662631\n",
      "0m 46s (- 0m 8s) (50400 84%) 0.0480 0.0480\n",
      "Val Acc:  0.06797810227051378\n",
      "1m 0s (- 0m 11s) (50500 84%) 0.0404 0.0404\n",
      "Val Acc:  0.0676291267387569\n",
      "1m 13s (- 0m 13s) (50600 84%) 0.0429 0.0429\n",
      "Val Acc:  0.06888680682331323\n",
      "1m 27s (- 0m 16s) (50700 84%) 0.0408 0.0408\n",
      "Val Acc:  0.07011943362466991\n",
      "1m 41s (- 0m 18s) (50800 84%) 0.0431 0.0431\n",
      "Val Acc:  0.06813179255276919\n",
      "1m 55s (- 0m 20s) (50900 84%) 0.0438 0.0438\n",
      "Val Acc:  0.06589603094756603\n",
      "2m 8s (- 0m 22s) (51000 85%) 0.0413 0.0413\n",
      "Val Acc:  0.06773627009242773\n",
      "2m 22s (- 0m 24s) (51100 85%) 0.0395 0.0395\n",
      "Val Acc:  0.07258052104339004\n",
      "2m 36s (- 0m 26s) (51200 85%) 0.0412 0.0412\n",
      "Val Acc:  0.06905434950068592\n",
      "2m 50s (- 0m 28s) (51300 85%) 0.0427 0.0427\n",
      "Val Acc:  0.06715524130221456\n",
      "3m 4s (- 0m 30s) (51400 85%) 0.0440 0.0440\n",
      "Val Acc:  0.06733858187682927\n",
      "3m 18s (- 0m 32s) (51500 85%) 0.0435 0.0435\n",
      "Val Acc:  0.06624422745406627\n",
      "3m 31s (- 0m 34s) (51600 86%) 0.0405 0.0405\n",
      "Val Acc:  0.06684647085145116\n",
      "3m 45s (- 0m 36s) (51700 86%) 0.0416 0.0416\n",
      "Val Acc:  0.06722506421245635\n",
      "3m 59s (- 0m 37s) (51800 86%) 0.0426 0.0426\n",
      "Val Acc:  0.06668329134397209\n",
      "4m 13s (- 0m 39s) (51900 86%) 0.0447 0.0447\n",
      "Val Acc:  0.06700339433923364\n",
      "4m 27s (- 0m 41s) (52000 86%) 0.0385 0.0385\n",
      "Val Acc:  0.06568434056639672\n",
      "4m 41s (- 0m 42s) (52100 86%) 0.0443 0.0443\n",
      "Val Acc:  0.06827062816172838\n",
      "4m 55s (- 0m 44s) (52200 87%) 0.0382 0.0382\n",
      "Val Acc:  0.07183693494275212\n",
      "5m 9s (- 0m 45s) (52300 87%) 0.0397 0.0397\n",
      "Val Acc:  0.06544356317445636\n",
      "5m 23s (- 0m 46s) (52400 87%) 0.0359 0.0359\n",
      "Val Acc:  0.0667061713077128\n",
      "5m 37s (- 0m 48s) (52500 87%) 0.0374 0.0374\n",
      "Val Acc:  0.06662796894833446\n",
      "5m 51s (- 0m 49s) (52600 87%) 0.0401 0.0401\n",
      "Val Acc:  0.06613425454311073\n",
      "6m 4s (- 0m 50s) (52700 87%) 0.0391 0.0391\n",
      "Val Acc:  0.06911302382685244\n",
      "6m 18s (- 0m 51s) (52800 88%) 0.0375 0.0375\n",
      "Val Acc:  0.06719787660427391\n",
      "6m 32s (- 0m 52s) (52900 88%) 0.0425 0.0425\n",
      "Val Acc:  0.06661620563827456\n",
      "6m 46s (- 0m 53s) (53000 88%) 0.0396 0.0396\n",
      "Val Acc:  0.07002268390357494\n",
      "7m 0s (- 0m 54s) (53100 88%) 0.0391 0.0391\n",
      "Val Acc:  0.06820836143288761\n",
      "7m 14s (- 0m 55s) (53200 88%) 0.0379 0.0379\n",
      "Val Acc:  0.07334697364270687\n",
      "7m 28s (- 0m 56s) (53300 88%) 0.0381 0.0381\n",
      "Val Acc:  0.06945391691662371\n",
      "7m 42s (- 0m 57s) (53400 89%) 0.0441 0.0441\n",
      "Val Acc:  0.06493040738999843\n",
      "7m 56s (- 0m 57s) (53500 89%) 0.0380 0.0380\n",
      "Val Acc:  0.07231721704080701\n",
      "8m 10s (- 0m 58s) (53600 89%) 0.0407 0.0407\n",
      "Val Acc:  0.06650430562347173\n",
      "8m 24s (- 0m 59s) (53700 89%) 0.0404 0.0404\n",
      "Val Acc:  0.06668658671993763\n",
      "8m 38s (- 0m 59s) (53800 89%) 0.0405 0.0405\n",
      "Val Acc:  0.06494957273080945\n",
      "8m 52s (- 1m 0s) (53900 89%) 0.0407 0.0407\n",
      "Val Acc:  0.06760110801458359\n",
      "9m 7s (- 1m 0s) (54000 90%) 0.0407 0.0407\n",
      "Val Acc:  0.07009771991893649\n",
      "9m 21s (- 1m 1s) (54100 90%) 0.0389 0.0389\n",
      "Val Acc:  0.06673736789263784\n",
      "9m 35s (- 1m 1s) (54200 90%) 0.0382 0.0382\n",
      "Val Acc:  0.06919964941591024\n",
      "9m 49s (- 1m 1s) (54300 90%) 0.0394 0.0394\n",
      "Val Acc:  0.06923965663090348\n",
      "10m 3s (- 1m 2s) (54400 90%) 0.0401 0.0401\n",
      "Val Acc:  0.06579051550477744\n",
      "10m 17s (- 1m 2s) (54500 90%) 0.0397 0.0397\n",
      "Val Acc:  0.06970973636582493\n",
      "10m 31s (- 1m 2s) (54600 91%) 0.0384 0.0384\n",
      "Val Acc:  0.0660359667185694\n",
      "10m 45s (- 1m 2s) (54700 91%) 0.0358 0.0358\n",
      "Val Acc:  0.06669579399190843\n",
      "10m 59s (- 1m 2s) (54800 91%) 0.0381 0.0381\n",
      "Val Acc:  0.0679728341512382\n",
      "11m 14s (- 1m 2s) (54900 91%) 0.0392 0.0392\n",
      "Val Acc:  0.06848652137443423\n",
      "11m 28s (- 1m 2s) (55000 91%) 0.0388 0.0388\n",
      "Val Acc:  0.07055349505273625\n",
      "11m 42s (- 1m 2s) (55100 91%) 0.0371 0.0371\n",
      "Val Acc:  0.07001596402749419\n",
      "11m 56s (- 1m 2s) (55200 92%) 0.0380 0.0380\n",
      "Val Acc:  0.06807914812397212\n",
      "12m 10s (- 1m 2s) (55300 92%) 0.0374 0.0374\n",
      "Val Acc:  0.0641045387275517\n",
      "12m 24s (- 1m 1s) (55400 92%) 0.0402 0.0402\n",
      "Val Acc:  0.07047851291205734\n",
      "12m 39s (- 1m 1s) (55500 92%) 0.0384 0.0384\n",
      "Val Acc:  0.07117724356986582\n",
      "12m 53s (- 1m 1s) (55600 92%) 0.0393 0.0393\n",
      "Val Acc:  0.06805225631967186\n",
      "13m 7s (- 1m 0s) (55700 92%) 0.0393 0.0393\n",
      "Val Acc:  0.06885866737365723\n",
      "13m 21s (- 1m 0s) (55800 93%) 0.0370 0.0370\n",
      "Val Acc:  0.07105398473888636\n",
      "13m 35s (- 0m 59s) (55900 93%) 0.0376 0.0376\n",
      "Val Acc:  0.06668563386425376\n",
      "13m 49s (- 0m 59s) (56000 93%) 0.0415 0.0415\n",
      "Val Acc:  0.06911579110473394\n",
      "14m 3s (- 0m 58s) (56100 93%) 0.0369 0.0369\n",
      "Val Acc:  0.07001403166539967\n",
      "14m 18s (- 0m 58s) (56200 93%) 0.0388 0.0388\n",
      "Val Acc:  0.070626371152699\n",
      "14m 32s (- 0m 57s) (56300 93%) 0.0382 0.0382\n",
      "Val Acc:  0.0650333794709295\n",
      "14m 46s (- 0m 56s) (56400 94%) 0.0417 0.0417\n",
      "Val Acc:  0.06943064728938043\n",
      "15m 0s (- 0m 55s) (56500 94%) 0.0385 0.0385\n",
      "Val Acc:  0.06726915339007973\n",
      "15m 14s (- 0m 54s) (56600 94%) 0.0357 0.0357\n",
      "Val Acc:  0.07233323426917196\n",
      "15m 28s (- 0m 54s) (56700 94%) 0.0366 0.0366\n",
      "Val Acc:  0.06973646652605385\n",
      "15m 42s (- 0m 53s) (56800 94%) 0.0350 0.0350\n",
      "Val Acc:  0.07484254943579435\n",
      "15m 56s (- 0m 52s) (56900 94%) 0.0356 0.0356\n",
      "Val Acc:  0.07313587268814445\n",
      "16m 10s (- 0m 51s) (57000 95%) 0.0341 0.0341\n",
      "Val Acc:  0.06907524341717362\n",
      "16m 25s (- 0m 50s) (57100 95%) 0.0365 0.0365\n",
      "Val Acc:  0.07020269686728715\n",
      "16m 39s (- 0m 48s) (57200 95%) 0.0331 0.0331\n",
      "Val Acc:  0.07205968558788299\n",
      "16m 53s (- 0m 47s) (57300 95%) 0.0386 0.0386\n",
      "Val Acc:  0.06980757148191333\n",
      "17m 7s (- 0m 46s) (57400 95%) 0.0370 0.0370\n",
      "Val Acc:  0.07342744580842554\n",
      "17m 21s (- 0m 45s) (57500 95%) 0.0379 0.0379\n",
      "Val Acc:  0.07002269560843706\n",
      "17m 35s (- 0m 43s) (57600 96%) 0.0358 0.0358\n",
      "Val Acc:  0.06676459120586514\n",
      "17m 49s (- 0m 42s) (57700 96%) 0.0383 0.0383\n",
      "Val Acc:  0.06932823076844215\n",
      "18m 3s (- 0m 41s) (57800 96%) 0.0361 0.0361\n",
      "Val Acc:  0.06791133484989405\n",
      "18m 17s (- 0m 39s) (57900 96%) 0.0369 0.0369\n",
      "Val Acc:  0.06977507951855659\n",
      "18m 32s (- 0m 38s) (58000 96%) 0.0335 0.0335\n",
      "Val Acc:  0.07129332113265992\n",
      "18m 46s (- 0m 36s) (58100 96%) 0.0339 0.0339\n",
      "Val Acc:  0.06869147485028952\n",
      "19m 0s (- 0m 35s) (58200 97%) 0.0372 0.0372\n",
      "Val Acc:  0.06672700157202781\n",
      "19m 14s (- 0m 33s) (58300 97%) 0.0388 0.0388\n",
      "Val Acc:  0.07058647603541612\n",
      "19m 28s (- 0m 32s) (58400 97%) 0.0338 0.0338\n",
      "Val Acc:  0.06902371039241552\n",
      "19m 42s (- 0m 30s) (58500 97%) 0.0385 0.0385\n",
      "Val Acc:  0.06695463726948947\n",
      "19m 56s (- 0m 28s) (58600 97%) 0.0376 0.0376\n",
      "Val Acc:  0.0698361455798149\n",
      "20m 11s (- 0m 26s) (58700 97%) 0.0361 0.0361\n",
      "Val Acc:  0.07011861570179462\n",
      "20m 25s (- 0m 25s) (58800 98%) 0.0338 0.0338\n",
      "Val Acc:  0.06910059873946012\n",
      "20m 39s (- 0m 23s) (58900 98%) 0.0308 0.0308\n",
      "Val Acc:  0.06708373814448715\n",
      "20m 53s (- 0m 21s) (59000 98%) 0.0389 0.0389\n",
      "Val Acc:  0.07002149737998843\n",
      "21m 7s (- 0m 19s) (59100 98%) 0.0364 0.0364\n",
      "Val Acc:  0.06990867614001035\n",
      "21m 21s (- 0m 17s) (59200 98%) 0.0370 0.0370\n",
      "Val Acc:  0.06960917530208827\n",
      "21m 35s (- 0m 15s) (59300 98%) 0.0370 0.0370\n",
      "Val Acc:  0.07107353781908751\n",
      "21m 49s (- 0m 13s) (59400 99%) 0.0362 0.0362\n",
      "Val Acc:  0.06768380379118025\n",
      "22m 3s (- 0m 11s) (59500 99%) 0.0383 0.0383\n",
      "Val Acc:  0.07083113242685794\n",
      "22m 18s (- 0m 8s) (59600 99%) 0.0357 0.0357\n",
      "Val Acc:  0.07087098745070398\n",
      "22m 32s (- 0m 6s) (59700 99%) 0.0326 0.0326\n",
      "Val Acc:  0.06736708951368928\n",
      "22m 46s (- 0m 4s) (59800 99%) 0.0301 0.0301\n",
      "Val Acc:  0.07177108057960868\n",
      "23m 0s (- 0m 2s) (59900 99%) 0.0357 0.0357\n",
      "Val Acc:  0.06957705482654274\n",
      "23m 14s (- -1m 59s) (60000 100%) 0.0376 0.0376\n",
      "Val Acc:  0.07204381707683205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5caec60eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5caec5ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialing hyperparam containers\n",
    "learning_rates = [0.007]\n",
    "length_regs = [0.0003]\n",
    "continuity_regs = [0.0006]\n",
    "learning_rate_decays = [0.95]\n",
    "num_epochs = 60\n",
    "gen_hidden_dim = 200\n",
    "gen_num_layers = 1\n",
    "gen_s_size = 30\n",
    "\n",
    "# load_dict=None\n",
    "for lrate_decay in learning_rate_decays:\n",
    "\tfor length_reg in length_regs:\n",
    "\t\tfor continuity_reg in continuity_regs:\n",
    "\t\t\tfor l_rate in learning_rates:\n",
    "\t\t\t\tgenerator = Generator.Generator(gen_hidden_dim, gen_num_layers, gen_s_size, pretrained_embeddings, 'LSTM')\n",
    "\t\t\t\t# fill encoder parameters\n",
    "\t\t\t\tencoder = Encoder.Encoder(200, 2, pretrained_embeddings, 'LSTM', dropout=0.1)\n",
    "\t\t\t\tencoder.float()\n",
    "\t\t\t\tgenerator.float()\n",
    "                \n",
    "\t\t\t\tif (use_cuda):\n",
    "\t\t\t\t\tencoder.cuda()\n",
    "\t\t\t\t\tgenerator.cuda()\n",
    "                    \n",
    "\t\t\t\ttrainIters(X, ratings, X_val, ratings_val, encoder, generator, \n",
    "\t\t\t\t\t\t\tlearning_rate=l_rate, learning_rate_decay=lrate_decay, num_epochs=num_epochs, \n",
    "\t\t\t\t\t\t\tlength_reg=length_reg, continuity_reg=continuity_reg, \\\n",
    "                            print_every=100,val_every=100,load_dict=load_dict, print_grad_every=-1, \\\n",
    "                          save_folder='only_enc2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toy_X = [[2], [1]]\n",
    "toy_ratings = [[0.2], [0.1]]\n",
    "toy_X_val = [[2]]\n",
    "toy_ratings_val = [[0.2]]\n",
    "load_dict=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'toy/chkpt_49900False'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-49fb2fa8dd05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mload_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'toy/chkpt_49900False'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ubuntu1604/temp/code/pytorch_venv/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'toy/chkpt_49900False'"
     ]
    }
   ],
   "source": [
    "load_dict = torch.load('toy/chkpt_49900False')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0m 0s (- 3m 58s) (100 0%) 0.0877 0.0877\n",
      "0m 0s (- 4m 8s) (200 0%) 0.0050 0.0050\n",
      "0m 0s (- 4m 7s) (300 0%) 0.0113 0.0113\n",
      "0m 0s (- 4m 11s) (400 0%) 0.0074 0.0074\n",
      "0m 0s (- 4m 15s) (500 0%) 0.0080 0.0080\n",
      "0m 0s (- 4m 15s) (600 0%) 0.0084 0.0084\n",
      "0m 0s (- 4m 12s) (700 0%) 0.0082 0.0082\n",
      "0m 1s (- 4m 9s) (800 0%) 0.0049 0.0049\n",
      "0m 1s (- 4m 6s) (900 0%) 0.0057 0.0057\n",
      "h2o.weight\n",
      "tensor(1.00000e-20 *\n",
      "       1.3882, device='cuda:0')\n",
      "tensor(1.00000e-21 *\n",
      "       1.3021, device='cuda:0')\n",
      "h2o.bias\n",
      "tensor(0.1455, device='cuda:0')\n",
      "tensor(0.1090, device='cuda:0')\n",
      "i2h.weight\n",
      "tensor(1.00000e-21 *\n",
      "       7.8079, device='cuda:0')\n",
      "tensor(1.00000e-21 *\n",
      "       3.0252, device='cuda:0')\n",
      "i2h.bias\n",
      "tensor(1.00000e-21 *\n",
      "       3.6645, device='cuda:0')\n",
      "tensor(1.00000e-21 *\n",
      "       1.5127, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu1604/temp/code/pytorch_venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m                 \u001b[0mident\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu1604/temp/code/pytorch_venv/lib/python3.5/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[1;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[0;32m    730\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 731\u001b[1;33m             \u001b[0mmsg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu1604/temp/code/pytorch_venv/lib/python3.5/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[1;34m(self, flags, copy, track)\u001b[0m\n\u001b[0;32m    357\u001b[0m         \"\"\"\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[1;31m# have first part already, only loop while more to receive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:6971)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:6763)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:1931)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu1604/temp/code/pytorch_venv/lib/python3.5/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:7222)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-7c72657e9761>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m                                         \u001b[0mlength_reg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlength_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinuity_reg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontinuity_reg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                                         \u001b[0mprint_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mload_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_grad_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                                        save_folder='toy/')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-86-71b184ec6ff4>\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(X, ratings, X_val, ratings_val, encoder, generator, learning_rate, learning_rate_decay, num_epochs, length_reg, continuity_reg, load_dict, print_every, plot_every, val_every, print_grad_every, save_folder)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mcur_tot_iters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0miters_per_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprint_grad_every\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcur_tot_iters\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_grad_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mcur_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_encoder_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratings_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m                                                    \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinuity_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mcur_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_encoder_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratings_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m                                                \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinuity_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-a7fbb5c94da0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, ratings, encoder, generator, encoder_optimizer, generator_optimizer, length_reg, continuity_reg, print_grad_norm)\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu1604/temp/code/pytorch_venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu1604/temp/code/pytorch_venv/lib/python3.5/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    732\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [0.5]\n",
    "length_regs = [0.0003]\n",
    "continuity_regs = [0.0006]\n",
    "learning_rate_decays = [1]\n",
    "num_epochs = 100000\n",
    "gen_hidden_dim = 200\n",
    "gen_num_layers = 1\n",
    "gen_s_size = 30\n",
    "\n",
    "# load_dict=None\n",
    "generator=None\n",
    "for lrate_decay in learning_rate_decays:\n",
    "    for length_reg in length_regs:\n",
    "        for continuity_reg in continuity_regs:\n",
    "            for l_rate in learning_rates:\n",
    "                generator = Generator.Generator(gen_hidden_dim, gen_num_layers, gen_s_size, pretrained_embeddings, 'LSTM')\n",
    "                encoder = TempEncoder.TempEncoder(1, 1, pretrained_embeddings, 'RNN', dropout=0.1)\n",
    "                encoder.float()\n",
    "                generator.float()\n",
    "    \n",
    "                if (use_cuda):\n",
    "                    encoder.cuda()\n",
    "                    generator.cuda()\n",
    "                trainIters(toy_X, toy_ratings, toy_X_val, toy_ratings_val, encoder, generator, \\\n",
    "                                        learning_rate=l_rate, learning_rate_decay=lrate_decay, num_epochs=num_epochs, \n",
    "                                        length_reg=length_reg, continuity_reg=continuity_reg, \\\n",
    "                                        print_every=100,val_every=1000,load_dict=load_dict, print_grad_every=-1, \\\n",
    "                                       save_folder='toy/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
