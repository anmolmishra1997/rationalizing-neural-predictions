{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import random\n",
    "\n",
    "import Generator\n",
    "import Encoder\n",
    "# Global defs\n",
    "\n",
    "# iters_per_epoch should also be shifted here ?\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "batch_size = 64\n",
    "\n",
    "MASTER_MAX_LEN = 300\n",
    "MASTER_MAX_VAL_LEN = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function defs\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def asMinutes(s):\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "\tnow = time.time()\n",
    "\ts = now - since\n",
    "\tes = s / (percent + 1e-8)\n",
    "\trs = es - s\n",
    "\treturn '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Starting of the Main code \n",
    "# ** ** \n",
    "# ** ** \n",
    "import re\n",
    "\n",
    "# Convert string to vector of floats\n",
    "def convert_to_float(string): # string with float values separated by spaces\n",
    "\tlis = string.split()\n",
    "\tlis_rating = [ float(value) for value in lis]\n",
    "\treturn lis_rating\n",
    "\n",
    "# Unique index for words\n",
    "index = 0\n",
    "def get_index():\n",
    "\tglobal index\n",
    "\tto_ret = index\n",
    "\tindex += 1\n",
    "\treturn to_ret\n",
    "\n",
    "# Dictionaries\n",
    "dict_ind2vec = {}\n",
    "dict_ind2str = {}\n",
    "dict_str2ind = {}\n",
    "\n",
    "def get_list_of_indices(string):\n",
    "\tlis_words = string.split()\n",
    "\t# lis_ret = [ for word in lis_words]\n",
    "\tlis_ret = []\n",
    "\tfor word in lis_words:\n",
    "\t\ttry:\n",
    "\t\t\tind_append = dict_str2ind[word]\n",
    "\t\t\tlis_ret.append(ind_append)\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\t\t\t# ind_append = \n",
    "\t\t\t# print(\"THERE IT IS!\", word)\n",
    "\t# print(\"About to return\")\n",
    "\treturn lis_ret\n",
    "\n",
    "## **\n",
    "## **\n",
    "# read the word2vec representations\n",
    "\n",
    "with open('../review+wiki.filtered.200.txt') as f:\n",
    "\twordvecs = f.readlines()\n",
    "\n",
    "first_pair = wordvecs[0].split(\" \", 1)\n",
    "first_vec = convert_to_float(first_pair[1])\n",
    "dim_vecSpace = len(first_vec) # Dimension of the vector space in which we are\n",
    "\n",
    "# add stuff for EOS, Blank\n",
    "# at index = 0, 1\n",
    "\n",
    "eos_index = get_index()\n",
    "dict_str2ind[\"<EOS>\"] = eos_index\n",
    "dict_ind2str[eos_index] = \"<EOS>\"\n",
    "dict_ind2vec[eos_index] = [1.0]*dim_vecSpace\n",
    "\n",
    "blk_index = get_index()\n",
    "dict_str2ind[\"<BLANK>\"] = blk_index\n",
    "dict_ind2str[blk_index] = \"<BLANK>\"\n",
    "dict_ind2vec[blk_index] = [0.0]*dim_vecSpace\n",
    "\n",
    "\n",
    "for elem in wordvecs:\n",
    "\tliss = elem.split(\" \", 1) # split on the first space\n",
    "\tword_str = liss[0]\n",
    "\tword_vec = convert_to_float(liss[1])\n",
    "\t\n",
    "\there_index = get_index()\n",
    "\tdict_str2ind[word_str] = here_index\n",
    "\tdict_ind2str[here_index] = word_str\n",
    "\tdict_ind2vec[here_index] = word_vec\n",
    "\n",
    "# CHKING\n",
    "# print( dict_str2ind['a'] )\n",
    "\n",
    "## **\n",
    "## **\n",
    "# read the data\n",
    "\n",
    "with open('../reviews.aspect0.train.txt') as f:\n",
    "\ttrain_data = f.readlines()\n",
    "\n",
    "rating_regex = re.compile('\\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d\\t') # Exactly matches only the ratings\n",
    "\n",
    "# extract ratings - # each rating is a scalar value # NO ::: each rating is a list of 5 values\n",
    "ratings = [ float( re.findall(rating_regex, review)[0][0] ) for review in train_data ]\n",
    "\n",
    "# extract reviews\n",
    "reviews_str = [ rating_regex.sub('', review) for review in train_data ]\n",
    "reviews = [ get_list_of_indices( review_str ) for review_str in reviews_str ]\n",
    "X = reviews\n",
    "total_size = len(X)\n",
    "\n",
    "divide_train = int( (4*total_size)/5 )\n",
    "train_indices_of_X = sorted( random.sample( range(total_size), divide_train ) )\n",
    "\n",
    "X_train = []\n",
    "X_val = []\n",
    "ratings_train = []\n",
    "ratings_val = []\n",
    "for i in range(total_size):\n",
    "\tif i in train_indices_of_X:\n",
    "\t\tX_train.append(X[i])\n",
    "\t\tratings_train.append(ratings[i])\n",
    "\telse:\n",
    "\t\tX_val.append(X[i])\n",
    "\t\tratings_val.append(ratings[i])\n",
    "\n",
    "X = X_train\n",
    "ratings = ratings_train\n",
    "\n",
    "num_train_examples = len(X) # we also assume len(X) = len(ratings)\n",
    "num_val_examples = len(X_val)\n",
    "# read validation data\n",
    "\n",
    "# ** ** \n",
    "# ** ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(X, ratings, encoder, generator):\n",
    "\n",
    "    # iterate through X_val and pass to generator->encoder to get mse_error and compare it to truth\n",
    "    num_val_examples = len(X)\n",
    "    X_val_size = num_val_examples\n",
    "    num_iters = X_val_size // (batch_size)\n",
    "    total_loss = 0.0\n",
    "    for iters in range(num_iters):\n",
    "\n",
    "        # get X_batch, ratings_batch\n",
    "        # This sampling also preserves the order\n",
    "        X_bch = []\n",
    "        ratings_bch = []\n",
    "\n",
    "        _ = [ ( X_bch.append(X[i]) , ratings_bch.append(ratings[i]) ) for i in sorted(random.sample(range(num_val_examples), batch_size)) ]\n",
    "\n",
    "        # almost done here - make all the reviews of equal length now\n",
    "\n",
    "        maxlen_rev = max(X_bch, key=len)\n",
    "        maxlen = len(maxlen_rev)\n",
    "\n",
    "        max_seq_len = min(maxlen, MASTER_MAX_VAL_LEN)\n",
    "        \n",
    "        \n",
    "        X_bach = np.empty([batch_size,max_seq_len])\n",
    "        ratings_bach = np.empty([batch_size,1])\n",
    "\n",
    "        for iterr in range(batch_size):\n",
    "            currentlen = len(X_bch[iterr])\n",
    "            if (currentlen < max_seq_len):\n",
    "                zero_count = max_seq_len - currentlen\n",
    "                X_bch[iterr].extend([0]*zero_count)\n",
    "            else:\n",
    "                X_bch[iterr] = X_bch[iterr][0:max_seq_len]\n",
    "            # X_bch[iterr] is now a list containing indices of words\n",
    "            # Convert it into a Variable ?\n",
    "            to_append = np.array( X_bch[iterr] )\n",
    "            X_bach[iterr] = to_append\n",
    "            to_append = np.array( ratings_bch[iterr] )\n",
    "            ratings_bach[iterr] = to_append\n",
    "        # X_bach is a 2d numpy array of size :: batch_size X maxlen\n",
    "\n",
    "        if (use_cuda):\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor).cuda()\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor).cuda()\n",
    "        else:\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor)\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor)\n",
    "\n",
    "        X_batch = Variable(X_bach_tensor)\n",
    "        ratings_batch = Variable(ratings_bach_tensor)\n",
    "\n",
    "        init_hidden = generator.initHidden(batch_size, use_cuda)\n",
    "        z_sample = generator.sample(X_batch, init_hidden, use_cuda)\n",
    "\n",
    "        ratings_pred = encoder(X_batch, z_sample, False)\n",
    "        encoderLoss = nn.MSELoss(reduce=False)\n",
    "        encoder_loss = encoderLoss(ratings_pred, ratings_batch.squeeze(1))\n",
    "\n",
    "        total_loss += float(torch.sum(encoder_loss))\n",
    "\n",
    "    return total_loss / X_val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function - here's some ingenuity\n",
    "# one iteration of training\n",
    "def train(X, ratings, encoder, generator, encoder_optimizer, generator_optimizer, \\\n",
    "          length_reg, continuity_reg, print_grad_norm):\n",
    "    # X - single batch\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    generator_optimizer.zero_grad()\n",
    "\n",
    "    encoderLoss = nn.MSELoss(reduce=False)\n",
    "\n",
    "    mean_cost = 0.0\n",
    "    mean_encoder_cost = 0.0\n",
    "    for i in range(num_samples):\n",
    "        init_hidden = generator.initHidden(batch_size, use_cuda)\n",
    "        z_sample = generator.sample(X, init_hidden, use_cuda)\n",
    "#         z_sample = Variable(torch.zeros((batch_size, X.shape[1])))\n",
    "        z_sample = z_sample.detach()\n",
    "\n",
    "        ratings_pred = encoder(X, z_sample)\n",
    "        encoder_loss = encoderLoss(ratings_pred, ratings.squeeze(1))\n",
    "\n",
    "        init_hidden = generator.initHidden(batch_size, use_cuda)\n",
    "        length_cost, continuity_cost = generator.loss(z_sample)\n",
    "\n",
    "        cost = encoder_loss + length_cost * length_reg + continuity_cost * continuity_reg\n",
    "        mean_cost += float(torch.mean(cost))\n",
    "        mean_encoder_cost += float(torch.mean(encoder_loss))\n",
    "\n",
    "        log_prob = generator.logProb(X, z_sample, init_hidden, use_cuda)\n",
    "\n",
    "        log_prob.backward(1.0 / (num_samples * batch_size) * cost)\n",
    "#         cost.backward(torch.Tensor([1.0 / (batch_size * num_samples)] * batch_size))\n",
    "        cost1 = torch.mean(cost)\n",
    "        cost1 /= num_samples\n",
    "        cost1.backward()\n",
    "        \n",
    "        if (print_grad_norm):\n",
    "            for name, param in encoder.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    print(name)\n",
    "                    print(param.data.norm())\n",
    "                    print (param.grad.data.norm())\n",
    "\n",
    "            input()\n",
    "    encoder_optimizer.step()\n",
    "    generator_optimizer.step()\n",
    "\n",
    "    mean_cost /= num_samples\n",
    "    mean_encoder_cost /= num_samples\n",
    "    return mean_cost, mean_encoder_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(X, ratings, X_val, ratings_val, encoder, generator, learning_rate, learning_rate_decay, num_epochs, \\\n",
    "               length_reg, continuity_reg, load_dict=None, print_every=1000, plot_every=100, val_every=1000, \\\n",
    "              print_grad_every=-1, save_folder='', weight_decay=0):\n",
    "\n",
    "    num_train_examples = len(X)\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0.0\n",
    "    print_encoder_loss_total = 0.0\n",
    "    plot_loss_total = 0.0\n",
    "    best_val_acc = float(\"inf\")\n",
    "    if load_dict is not None:\n",
    "        encoder = load_dict['encoder_model']\n",
    "        generator = load_dict['generator_model']\n",
    "        \n",
    "        cur_tot_iters = load_dict['tot_iter']\n",
    "    else:\n",
    "        cur_tot_iters = 0\n",
    "        \n",
    "    enc_param_list = []\n",
    "    for param in encoder.parameters():\n",
    "        if (param.requires_grad):\n",
    "            enc_param_list.append(param)\n",
    "    encoder_optimizer = optim.Adam(enc_param_list, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    gen_param_list = []\n",
    "    for param in generator.parameters():\n",
    "        if (param.requires_grad):\n",
    "            gen_param_list.append(param)\n",
    "    generator_optimizer = optim.Adam(gen_param_list, lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    if (load_dict is not None):\n",
    "        encoder_optimizer.load_state_dict(load_dict['encoder_optimizer'])\n",
    "        for param_group in encoder_optimizer.param_groups:\n",
    "            pass\n",
    "#             param_group['weight_decay'] = weight_decay\n",
    "#             param_group['lr'] = learning_rate\n",
    "        generator_optimizer.load_state_dict(load_dict['generator_optimizer'])\n",
    "#         encoder_scheduler.load_state_dict(load_dict['encoder_scheduler'])\n",
    "#         generator_scheduler.load_state_dict(load_dict['generator_scheduler'])\n",
    "        \n",
    "        \n",
    "    \n",
    "    # set iters_per_epoch\n",
    "    iters_per_epoch = num_train_examples // batch_size\n",
    "    n_iters = iters_per_epoch * num_epochs\n",
    "\n",
    "    encoder_scheduler = optim.lr_scheduler.StepLR(encoder_optimizer, 1, learning_rate_decay)\n",
    "    generator_scheduler = optim.lr_scheduler.StepLR(generator_optimizer, 1, learning_rate_decay)\n",
    "\n",
    "    \n",
    "    position_set = False\n",
    "    for epoch in range(num_epochs):\n",
    "        if (position_set):\n",
    "            pass\n",
    "#             encoder_scheduler.step()\n",
    "#             generator_scheduler.step()\n",
    "        for iter_num in range(iters_per_epoch):\n",
    "            if (cur_tot_iters >= epoch * iters_per_epoch + iter_num + 1):\n",
    "                continue\n",
    "            \n",
    "            position_set = True\n",
    "            \n",
    "            # randomly choose sample from X and make them equal length\n",
    "            # This sampling also preserves the order\n",
    "            X_bch = []\n",
    "            ratings_bch = []\n",
    "\n",
    "            _ = [ ( X_bch.append(X[i]) , ratings_bch.append(ratings[i]) ) for i in sorted(random.sample(range(num_train_examples), batch_size)) ]\n",
    "\n",
    "            # almost done here - make all the reviews of equal length now\n",
    "\n",
    "            maxlen_rev = max(X_bch, key=len)\n",
    "            maxlen = len(maxlen_rev)\n",
    "\n",
    "            max_seq_len = min(maxlen, MASTER_MAX_LEN)\n",
    "            \n",
    "            X_bach = np.empty([batch_size,max_seq_len])\n",
    "            ratings_bach = np.empty([batch_size,1])\n",
    "\n",
    "            for iterr in range(batch_size):\n",
    "                currentlen = len(X_bch[iterr])\n",
    "                if (currentlen < max_seq_len):\n",
    "                    zero_count = max_seq_len - currentlen\n",
    "                    X_bch[iterr].extend([0]*zero_count)\n",
    "                else:\n",
    "                    X_bch[iterr] = X_bch[iterr][0:max_seq_len]\n",
    "                # X_bch[iterr] is now a list containing indices of words\n",
    "                # Convert it into a Variable ?\n",
    "                to_append = np.array( X_bch[iterr] )\n",
    "    # \t\t\t\tX_bach = np.append(X_bach, [to_append], axis = 0)\n",
    "                X_bach[iterr] = to_append\n",
    "                to_append = np.array( ratings_bch[iterr] )\n",
    "    # \t\t\t\tratings_bach = np.append(ratings_bach, to_append, axis = 0)\n",
    "                ratings_bach[iterr] = to_append\n",
    "            # X_bach is a 2d numpy array of size :: batch_size X maxlen\n",
    "\n",
    "            if (use_cuda):\n",
    "                X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor).cuda()\n",
    "                ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor).cuda()\n",
    "            else:\n",
    "                X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor)\n",
    "                ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor)\n",
    "            X_batch = Variable(X_bach_tensor)\n",
    "            ratings_batch = Variable(ratings_bach_tensor)\n",
    "            # call train with this batch\n",
    "            cur_tot_iters = iter_num + 1 + epoch * iters_per_epoch\n",
    "            if (print_grad_every > 0 and cur_tot_iters % print_grad_every == 0):\n",
    "                cur_loss, cur_encoder_loss = train(X_batch, ratings_batch, encoder, generator, \\\n",
    "                                                   encoder_optimizer, generator_optimizer, length_reg, continuity_reg, True)\n",
    "            else:\n",
    "                cur_loss, cur_encoder_loss = train(X_batch, ratings_batch, encoder, generator, \\\n",
    "                                               encoder_optimizer, generator_optimizer, length_reg, continuity_reg, False)\n",
    "            \n",
    "#             cur_loss, cur_encoder_loss = train(X_batch, ratings_batch, encoder, generator, encoder_optimizer, generator_optimizer, length_reg, continuity_reg)\n",
    "\n",
    "            print_loss_total += cur_loss\n",
    "            print_encoder_loss_total += cur_encoder_loss\n",
    "            plot_loss_total += cur_loss\n",
    "\n",
    "            \n",
    "            if (iter_num + 1) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_encoder_loss_avg = print_encoder_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print_encoder_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f %.4f' % (timeSince(start, 1.0 * (cur_tot_iters) / n_iters),\n",
    "                                             cur_tot_iters, 1.0 * (cur_tot_iters) / n_iters * 100, print_loss_avg, print_encoder_loss_avg),flush=True)\n",
    "            \n",
    "            if (iter_num + 1) % val_every == 0:\n",
    "                val_acc = getAccuracy(X_val, ratings_val, encoder, generator)\n",
    "                print(\"Val Acc: \", val_acc)\n",
    "                if (val_acc < best_val_acc):\n",
    "                    best_val_acc = val_acc\n",
    "                    best = True\n",
    "                else:\n",
    "                    best = False\n",
    "                    \n",
    "                save_dict = {}\n",
    "                save_dict['encoder_model'] = encoder\n",
    "                save_dict['generator_model'] = generator\n",
    "                save_dict['encoder_optimizer'] = encoder_optimizer.state_dict()\n",
    "                save_dict['generator_optimizer'] = generator_optimizer.state_dict()\n",
    "#                 save_dict['encoder_scheduler'] = encoder_scheduler\n",
    "#                 save_dict['generator_scheduler'] = generator_scheduler\n",
    "                \n",
    "                save_dict['tot_iter'] = cur_tot_iters\n",
    "                save_dict['val_acc'] = val_acc\n",
    "                save_dict['best_so_far'] = best\n",
    "                torch.save(save_dict, save_folder+'chkpt_'+str(cur_tot_iters)+str(best))\n",
    "\n",
    "            if iter_num % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining pretrained_embeddings\n",
    "pretrained_embeddings = np.empty([len(dict_ind2vec), dim_vecSpace])\n",
    "for key in sorted(dict_ind2vec.keys()):\n",
    "    vec_here = dict_ind2vec[key]\n",
    "    pretrained_embeddings[key] = np.array(vec_here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 22s (- 687m 24s) (100 0%) 0.3538 0.1934\n",
      "2m 43s (- 678m 14s) (200 0%) 0.3145 0.1863\n",
      "4m 3s (- 672m 38s) (300 0%) 0.2968 0.1815\n",
      "5m 22s (- 666m 30s) (400 0%) 0.2982 0.1869\n",
      "6m 42s (- 663m 57s) (500 1%) 0.2916 0.1881\n",
      "8m 2s (- 662m 19s) (600 1%) 0.2847 0.1893\n",
      "9m 23s (- 661m 28s) (700 1%) 0.2733 0.1827\n",
      "10m 44s (- 660m 23s) (800 1%) 0.2720 0.1847\n",
      "12m 7s (- 661m 3s) (900 1%) 0.2733 0.1868\n",
      "13m 30s (- 661m 30s) (1000 2%) 0.2708 0.1833\n",
      "Val Acc:  0.18918006056547165\n",
      "17m 12s (- 764m 45s) (1100 2%) 0.2724 0.1863\n",
      "18m 33s (- 754m 58s) (1200 2%) 0.2737 0.1880\n",
      "19m 54s (- 746m 1s) (1300 2%) 0.2716 0.1855\n",
      "21m 16s (- 738m 23s) (1400 2%) 0.2674 0.1813\n",
      "22m 38s (- 732m 0s) (1500 3%) 0.2647 0.1802\n",
      "24m 0s (- 726m 24s) (1600 3%) 0.2730 0.1893\n",
      "25m 23s (- 721m 25s) (1700 3%) 0.2686 0.1854\n",
      "26m 45s (- 716m 21s) (1800 3%) 0.2627 0.1825\n",
      "28m 6s (- 711m 42s) (1900 3%) 0.2687 0.1885\n",
      "29m 28s (- 707m 20s) (2000 4%) 0.2660 0.1883\n",
      "Val Acc:  0.18890123265981673\n",
      "33m 8s (- 756m 7s) (2100 4%) 0.2610 0.1849\n",
      "34m 29s (- 749m 27s) (2200 4%) 0.2571 0.1816\n",
      "35m 49s (- 742m 49s) (2300 4%) 0.2468 0.1830\n",
      "37m 9s (- 736m 55s) (2400 4%) 0.1966 0.1827\n",
      "38m 30s (- 731m 45s) (2500 5%) 0.1930 0.1863\n",
      "39m 57s (- 728m 25s) (2600 5%) 0.1937 0.1868\n",
      "41m 23s (- 725m 6s) (2700 5%) 0.1892 0.1821\n",
      "42m 50s (- 722m 3s) (2800 5%) 0.1918 0.1862\n",
      "44m 16s (- 718m 57s) (2900 5%) 0.1902 0.1848\n",
      "45m 39s (- 715m 19s) (3000 6%) 0.1896 0.1837\n",
      "Val Acc:  0.1883103576004505\n",
      "49m 30s (- 749m 5s) (3100 6%) 0.1902 0.1867\n",
      "50m 53s (- 744m 10s) (3200 6%) 0.1888 0.1858\n",
      "52m 14s (- 739m 23s) (3300 6%) 0.1883 0.1857\n",
      "53m 37s (- 734m 58s) (3400 6%) 0.1871 0.1844\n",
      "55m 0s (- 730m 47s) (3500 7%) 0.1883 0.1855\n",
      "56m 21s (- 726m 29s) (3600 7%) 0.1903 0.1877\n",
      "57m 43s (- 722m 19s) (3700 7%) 0.1849 0.1825\n",
      "59m 6s (- 718m 33s) (3800 7%) 0.1885 0.1860\n",
      "60m 27s (- 714m 40s) (3900 7%) 0.1882 0.1859\n",
      "61m 48s (- 710m 52s) (4000 8%) 0.1876 0.1856\n",
      "Val Acc:  0.18514968061447143\n",
      "65m 26s (- 732m 38s) (4100 8%) 0.1867 0.1848\n",
      "66m 45s (- 727m 59s) (4200 8%) 0.1884 0.1865\n",
      "68m 6s (- 723m 50s) (4300 8%) 0.1865 0.1847\n",
      "69m 27s (- 719m 47s) (4400 8%) 0.1857 0.1842\n",
      "70m 44s (- 715m 20s) (4500 9%) 0.1853 0.1841\n",
      "72m 3s (- 711m 9s) (4600 9%) 0.1828 0.1814\n",
      "73m 21s (- 707m 2s) (4700 9%) 0.1828 0.1813\n",
      "74m 38s (- 702m 56s) (4800 9%) 0.1857 0.1846\n",
      "75m 55s (- 698m 51s) (4900 9%) 0.1828 0.1817\n",
      "77m 14s (- 695m 9s) (5000 10%) 0.1818 0.1807\n",
      "Val Acc:  0.18292209619283675\n",
      "80m 49s (- 711m 35s) (5100 10%) 0.1840 0.1830\n",
      "82m 9s (- 707m 48s) (5200 10%) 0.1825 0.1815\n",
      "83m 26s (- 703m 48s) (5300 10%) 0.1882 0.1873\n",
      "84m 45s (- 700m 4s) (5400 10%) 0.1949 0.1938\n",
      "86m 5s (- 696m 33s) (5500 11%) 0.1863 0.1852\n",
      "87m 25s (- 693m 6s) (5600 11%) 0.1841 0.1831\n",
      "88m 46s (- 689m 55s) (5700 11%) 0.1931 0.1923\n",
      "90m 6s (- 686m 44s) (5800 11%) 0.1873 0.1863\n",
      "91m 28s (- 683m 46s) (5900 11%) 0.1883 0.1874\n",
      "92m 50s (- 680m 52s) (6000 12%) 0.1839 0.1830\n",
      "Val Acc:  0.1861030076444149\n",
      "96m 35s (- 695m 7s) (6100 12%) 0.1857 0.1850\n",
      "97m 56s (- 691m 53s) (6200 12%) 0.1832 0.1824\n",
      "99m 17s (- 688m 47s) (6300 12%) 0.1821 0.1813\n",
      "100m 39s (- 685m 44s) (6400 12%) 0.1844 0.1835\n",
      "102m 2s (- 682m 50s) (6500 13%) 0.1817 0.1809\n",
      "103m 23s (- 679m 54s) (6600 13%) 0.1876 0.1869\n",
      "104m 44s (- 676m 54s) (6700 13%) 0.1821 0.1813\n",
      "106m 6s (- 674m 4s) (6800 13%) 0.1832 0.1824\n",
      "107m 27s (- 671m 13s) (6900 13%) 0.1835 0.1828\n",
      "108m 48s (- 668m 26s) (7000 14%) 0.1847 0.1840\n",
      "Val Acc:  0.18851559606194496\n",
      "112m 33s (- 680m 9s) (7100 14%) 0.1820 0.1813\n",
      "113m 55s (- 677m 11s) (7200 14%) 0.1857 0.1848\n",
      "115m 16s (- 674m 16s) (7300 14%) 0.1856 0.1848\n",
      "116m 37s (- 671m 25s) (7400 14%) 0.1832 0.1821\n",
      "117m 59s (- 668m 37s) (7500 15%) 0.1918 0.1910\n",
      "119m 22s (- 665m 58s) (7600 15%) 0.1849 0.1842\n",
      "120m 47s (- 663m 36s) (7700 15%) 0.1871 0.1864\n",
      "122m 13s (- 661m 15s) (7800 15%) 0.1851 0.1844\n",
      "123m 40s (- 659m 3s) (7900 15%) 0.1874 0.1868\n",
      "125m 7s (- 656m 54s) (8000 16%) 0.1863 0.1856\n",
      "Val Acc:  0.1889820113480091\n",
      "129m 27s (- 669m 39s) (8100 16%) 0.1854 0.1847\n",
      "130m 54s (- 667m 17s) (8200 16%) 0.1854 0.1849\n",
      "132m 20s (- 664m 54s) (8300 16%) 0.1871 0.1865\n",
      "133m 44s (- 662m 22s) (8400 16%) 0.1859 0.1853\n",
      "135m 7s (- 659m 42s) (8500 17%) 0.1870 0.1863\n",
      "136m 32s (- 657m 17s) (8600 17%) 0.1861 0.1853\n",
      "137m 55s (- 654m 43s) (8700 17%) 0.1860 0.1852\n",
      "139m 16s (- 652m 4s) (8800 17%) 0.1838 0.1830\n",
      "140m 39s (- 649m 32s) (8900 17%) 0.1784 0.1774\n",
      "142m 0s (- 646m 56s) (9000 18%) 0.1849 0.1839\n",
      "Val Acc:  0.18605808505415916\n",
      "145m 47s (- 655m 14s) (9100 18%) 0.1836 0.1828\n",
      "147m 8s (- 652m 30s) (9200 18%) 0.1887 0.1880\n",
      "148m 29s (- 649m 50s) (9300 18%) 0.1906 0.1900\n",
      "149m 50s (- 647m 13s) (9400 18%) 0.1902 0.1898\n",
      "151m 12s (- 644m 38s) (9500 19%) 0.1859 0.1854\n",
      "152m 35s (- 642m 8s) (9600 19%) 0.1805 0.1800\n",
      "153m 55s (- 639m 28s) (9700 19%) 0.1874 0.1869\n",
      "155m 16s (- 636m 58s) (9800 19%) 0.1855 0.1849\n",
      "156m 39s (- 634m 30s) (9900 19%) 0.1886 0.1880\n",
      "158m 0s (- 632m 1s) (10000 20%) 0.1849 0.1843\n",
      "Val Acc:  0.1890284321308136\n",
      "161m 56s (- 639m 45s) (10100 20%) 0.1839 0.1833\n",
      "164m 34s (- 642m 9s) (10200 20%) 0.1864 0.1860\n",
      "166m 0s (- 639m 53s) (10300 20%) 0.1879 0.1875\n",
      "167m 37s (- 638m 16s) (10400 20%) 0.1819 0.1817\n",
      "170m 14s (- 640m 27s) (10500 21%) 0.1836 0.1834\n",
      "171m 38s (- 637m 58s) (10600 21%) 0.1826 0.1823\n",
      "172m 58s (- 635m 19s) (10700 21%) 0.1815 0.1812\n"
     ]
    }
   ],
   "source": [
    "# Initialing hyperparam containers\n",
    "learning_rates = [0.000025883]\n",
    "length_regs = [0.0004]\n",
    "continuity_regs = [0.0008]\n",
    "learning_rate_decays = [1]\n",
    "weight_regs = [5e-6]\n",
    "num_epochs = 50\n",
    "gen_hidden_dim = 250\n",
    "gen_num_layers = 1\n",
    "gen_s_size = 30\n",
    "\n",
    "load_dict = None\n",
    "for lrate_decay in learning_rate_decays:\n",
    "    for length_reg in length_regs:\n",
    "        for continuity_reg in continuity_regs:\n",
    "            for l_rate in learning_rates:\n",
    "                for wt_reg in weight_regs:\n",
    "                    generator = Generator.Generator(gen_hidden_dim, gen_num_layers, gen_s_size, pretrained_embeddings, 'LSTM')\n",
    "                    # fill encoder parameters\n",
    "                    encoder = Encoder.Encoder(200, 2, pretrained_embeddings, 'LSTM', dropout=0.05)\n",
    "                    encoder.float()\n",
    "                    generator.float()\n",
    "                    \n",
    "                    if (use_cuda):\n",
    "                        encoder.cuda()\n",
    "                        generator.cuda()\n",
    "                    save_folder = \"enc_gen1_\" + str(wt_reg) + \"/\"\n",
    "                    trainIters(X, ratings, X_val, ratings_val, encoder, generator, \n",
    "                                learning_rate=l_rate, learning_rate_decay=lrate_decay, num_epochs=num_epochs, \n",
    "                                length_reg=length_reg, continuity_reg=continuity_reg, \n",
    "                               print_every=100,val_every=1000, load_dict=load_dict, print_grad_every=-1, \n",
    "                              save_folder=save_folder,weight_decay=wt_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder.Encoder(50, 1, pretrained_embeddings, 'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (10) : invalid device ordinal at torch/csrc/cuda/Module.cpp:88",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7f75aa2fe156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \"\"\"\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (10) : invalid device ordinal at torch/csrc/cuda/Module.cpp:88"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
